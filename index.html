<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta http-equiv="X-UA-Compatible" CONTENT="IE=edge" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<title>Naoto Ohsaka</title>

	<!-- Bootstrap CSS -->
	<link href="css/bootstrap.min.css" rel="stylesheet" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				tex2jax: {
					inlineMath: [ ['$','$'] ],
					displayMath: [ ['$$','$$'] ]
				}
			});
	</script>
	<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"> </script>

	<meta name="google-site-verification" content="9AHKTefKNmaaQ_D-OTrx9gcYwaA5kUWE_Q46e72Wlr0" />
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-1BDFTVC8SN"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'G-1BDFTVC8SN');
	</script>

	<style type="text/css">
		li {
			padding-top: 4px;
			padding-bottom: 4px;
		}

		.small-caps {
			font-variant-caps: small-caps;
		}

		.initial {
			font-variant-caps: initial;
		}

		a {
			text-decoration: none;
		}

		a:hover {
			text-decoration: underline;
		}

		.collapsing {
			transition: height 0.35s ease;
		}

		/* https://stackoverflow.com/questions/31793894/comic-sans-font-not-showing-on-mobile */
		@font-face {
			font-family: "Comic Sans MS";
			src: url("fonts/Comic_Sans_MS.ttf");
		}
	</style>
</head>


<body style='font-family: "Comic Sans MS", "Comic Sans";'>
	<div class="container m-auto py-3">
		<h1>Welcome to My Homepage!!</h1>
		<h2>Naoto Ohsaka</h2>
		<div><i class="bi bi-camera-fill"></i> <a href="./photo.jpg">Photo</a> (taken by <a href="https://sites.google.com/view/koyo-hayashi">Koyo Hayashi</a>, January 2024)</div>
		<div><i class="bi bi-person-circle"></i> <a href="./cv.pdf">Curriculum Vitae</a></div>
		<div><i class="bi bi-envelope"></i> Email: <a href="mailto:naoto.ohsaka@gmail.com">naoto.ohsaka@gmail.com</a> <small>(recommended) or</small> <a href="mailto:ohsaka_naoto@cyberagent.co.jp">ohsaka_naoto@cyberagent.co.jp</a> <small class="text-body-tertiary">(previously: ohsaka@is.s.u-tokyo.ac.jp, ohsaka@nec.com)</small></div>
		<div><i class="ai ai-google-scholar ai-lg"></i> Google Scholar: <a href="https://scholar.google.co.jp/citations?user=Qgkc9DgAAAAJ">Qgkc9DgAAAAJ</a></div>
		<div><i class="ai ai-orcid"></i> ORCID: <a href="https://orcid.org/0000-0001-9584-4764">0000-0001-9584-4764</a></div>
		<div><i class="ai ai-dblp ai-lg"></i> DBLP: <a href="https://dblp.org/pid/81/10779.html">81/10779</a></div>

		<hr>

		<h3>About Me</h3>
		<p>
			I am a research scientist at <a href="https://research.cyberagent.ai/">CyberAgent, Inc.</a> in Japan since December 2021.
			I have a broad interest in Theoretical Computer Science with an emphasis on
			<a href="https://en.wikipedia.org/wiki/Reconfiguration">Combinatorial Reconfiguration</a>,
			<a href="https://en.wikipedia.org/wiki/Hardness_of_approximation">Hardness of Approximation</a>,
			<a href="https://en.wikipedia.org/wiki/Probabilistically_checkable_proof">Probabilistically Checkable Proofs</a>, and
			<a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">Computational Complexity Theory</a>.
			I received my Ph.D. from Univesity of Tokyo in March 2018, and
			was a researcher at <a href="https://www.nec.com/en/global/rd/">NEC</a> in Japan from April 2018 to November 2021.
		</p>


		<hr>


		<h3>Selected Publications</h3>

		<ol>
			<li>
				Gap Preserving Reductions Between Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.conferences.uni-hamburg.de/event/272/page/153-home">40th International Symposium on Theoretical Aspects of Computer Science (STACS 2023)</a>. <br>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.STACS.2023.49" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2212.04207" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/STACS2023.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<div class="card card-body">
					<p class="card-text">
						<span class="badge text-bg-secondary"><i class="bi bi-magic"></i> Highlight</span>
						When I started studying reconfiguration problems, I was strongly motivated by <b>PSPACE</b>-hardness of approximation for them,
						which was posed as an open problem by Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (ISAAC 2008 & TCS 2011).
						Towards its resolution,
						I came up with the <i>Reconfiguration Inapproximability Hypothesis</i> (RIH) as a reconfiguration analogue of the PCP theorem.
						Starting from RIH, I created a series of gap-preserving reductions to give evidence that many reconfiguration problems are <b>PSPACE</b>-hard to approximate within a constant factor,
						including those of <span class="small-caps">3-SAT</span>, <span class="small-caps">2-CSP</span>, <span class="small-caps">Independent Set</span>, and <span class="small-caps">Vertex Cover</span>.
						Subsequently, in a joint work with <a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> (STOC 2024), RIH has been proven to be true.
					</p>
				</div>
			</li>
			<li>
				Gap Amplification for Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.siam.org/conferences/cm/conference/soda24">35th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2024)</a>. <br>
				<a class="btn btn-primary" href="https://doi.org/10.1137/1.9781611977912.54" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2310.14160" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/SODA2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<div class="card card-body">
					<p class="card-text">
						<span class="badge text-bg-secondary"><i class="bi bi-magic"></i> Highlight</span>
						One limitation of using RIH (STACS 2023) was that inapproximability factors are not explicit.
						Say, even 0.999…999-approximation for <span class="small-caps">2-CSP Reconfiguration</span> would not be ruled out.
						In the <b>NP</b> regime, the <i>parallel repetition theorem</i> can be used to derive many strong inapproximability results.
						Unfortunately, applying a “naive” parallel repetition to a reconfiguration analogue of the two-prover one-round game does not reduce its soundness error (Inf. Process. Lett., 2025).
						<br />
						In this paper, I was able to develop Dinur's style <i>gap amplification</i> for reconfiguration problems.
						Our main result is that <span class="small-caps">Maxmin 2-CSP Reconfiguration</span> is <b>PSPACE</b>-hard to approximate within a factor of 0.9942 only assuming RIH.
						As an application, I further demonstrated that <span class="small-caps">Minmax Set Cover Reconfiguration</span> and <span class="small-caps">Minmax Dominating Set Reconfiguration</span> are <b>PSPACE</b>-hard to approximate within a factor of 1.0029 under RIH.
					</p>
				</div>
			</li>

			<li>
				Probabilistically Checkable Reconfiguration Proofs and Inapproximability of Reconfiguration Problems.<br>
				<a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="http://acm-stoc.org/stoc2024/">56th Annual ACM Symposium on Theory of Computing (STOC 2024)</a>. <br>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3618260.3649667" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2401.00474" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-primary" href="https://eccc.weizmann.ac.il/report/2024/023" role="button">
					<i class="bi bi-file-pdf-fill"></i> ECCC
				</a>
				<a class="btn btn-success" href="./slides/STOC2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<a class="btn btn-danger" href="https://www.youtube.com/watch?v=nBY5s2k4Zx0" role="button">
					<i class="bi bi-youtube"></i> Video
				</a>
				<div class="card card-body">
					<p class="card-text">
						<span class="badge text-bg-secondary"><i class="bi bi-magic"></i> Highlight</span>
						We presented a new PCP-type characterization of <b>PSPACE</b>, namely <i>probabilistically checkable reconfiguration proofs</i> (PCRP).
						Our PCRP theorem states that any <b>PSPACE</b> computation can be encoded into an exponentially long reconfiguration sequence of polynomial length proofs,
						each of which is probabilistically checkable.
						Using the new characterization, we proved RIH, thereby affirmatively resolving the open problem posed by Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (ISAAC 2008 & TCS 2011).
						In particular, my previous results (STACS 2023 & SODA 2024) now hold unconditioanlly.
						<br />
						Independently, <a href="https://cskarthikcs.github.io/">Karthik C. S.</a> and <a href="https://pasin30055.github.io/">Pasin Manurangsi</a> also <a href="https://eccc.weizmann.ac.il/report/2024/007/">proved</a> RIH.
					</p>
				</div>
			</li>

			<li>
				Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration.<br>
				<a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://compose.ioc.ee/icalp2024/">51st EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2024)</a>. <br>

				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.ICALP.2024.85" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2402.12645" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-primary" href="https://eccc.weizmann.ac.il/report/2024/039" role="button">
					<i class="bi bi-file-pdf-fill"></i> ECCC
				</a>
				<a class="btn btn-success" href="./slides/ICALP2024_SetCoverReconf.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<div class="card card-body">
					<p class="card-text">
						<span class="badge text-bg-secondary"><i class="bi bi-magic"></i> Highlight</span>
						<span class="small-caps">Minmax Set Cover Reconfiguration</span> is a natural reconfiguration analogue of <span class="small-caps">Minimum Set Cover</span> and admits a simple $2$-factor approximation algorithm.
						We proved that this is optimal:
						<span class="small-caps">Minmax Set Cover Reconfiguration</span> is <b>PSPACE</b>-hard to approximate within a factor of $2-\frac{1}{\operatorname{polyloglog} N}$,
						where $N$ is the size of the universe.
						This is the first sharp result for <b>PSPACE</b>-hardness of approximating reconfiguration problems.
						Similar hardness results hold for <span class="small-caps">Dominating Set Reconfiguration</span> and <span class="small-caps">Hypergraph Vertex Cover</span>.
					</p>
				</div>
			</li>


			<li>
				Alphabet Reduction for Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://compose.ioc.ee/icalp2024/">51st EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2024)</a>. <br>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.ICALP.2024.113" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2402.10627" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/ICALP2024_ABCReductReconf.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<div class="card card-body">
					<p class="card-text">
						<span class="badge text-bg-secondary"><i class="bi bi-magic"></i> Highlight</span>
						Given the resolution of RIH,
						there was still not a “universal” PCRP system for <b>PSPACE</b> having <i>explicit values</i> of soundness, query complexity, and alphabet size.
						One reason was that gap amplification (SODA 2024) makes alphabet size gigantic (that depends on PCRP's gap).
						<br />
						In this paper, I developed alphabet reduction à la Dinur (J. ACM, 2007).
						Putting (STACS 2023, SODA 2024, STOC 2024) together, I obtained a PCRP system with
						soundness $\varepsilon = 10^{-18}$, query complexity $q = 2$, alphabet size $W = 2{,}000{,}000$.
						Even though this result is too far from optimal, I hope optimal PCRPs would be found in the near future.
					</p>
				</div>
			</li>

		</ol>


		<hr>


		<h3>Invited Talks</h3>
		<ol>
			<li>
				On Reconfigurability of Target Sets. <br>
				January 6th, 2022 @ <a href="https://core.dais.is.tohoku.ac.jp/en/report/past-event/detail/---id-96.html">20th CoRe Seminar</a>, Online (Japanese). <br>
			</li>

			<li>
				Reconfiguration Problems, Hardness of Approximation, and Gap Amplification: What Are They? <br>
				January 19th, 2024 @ <a href="https://kklab.nii.ac.jp/seminars/2023/12/reconfiguration-problems-hardness-of-approximation-and-gap-amplification-what-are-they.html">Kawarabayashi Lab Seminar</a>, Tokyo, Japan (Japanese). <br>
				<a class="btn btn-success" href="./slides/AFSA2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>

			<li>
				Gap Amplification for Reconfiguration Problems. <br>
				March 14th, 2024 @ <a href="https://ken.ieice.org/ken/program/index.php?mode=program&tgs_regid=a0f21f6f9b668b1ad7dab8960fbda529b9066cb460752da595c30e659e715485&tgid=COMP&layout=&lang=eng">Theoretical Foundations of Computing (COMP)</a>, Tokyo, Japan (Japanese). <br>
				<a class="btn btn-success" href="./slides/COMP2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>

			<li>
				On the Complexity of Approximating Reconfiguration Problems. <br>
				October 8th, 2024 @ <a href="http://www.dais.is.tohoku.ac.jp/take/core2024/">5th Combinatorial Reconfiguration Workshop</a>, Fukuoka, Japan. <br>
				<a class="btn btn-success" href="./slides/CoRe2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>

		</ol>


		<hr>


		<h3>Journal Articles & Refereed Conference Proceedings</h3>

		<ol reversed>


			<li>
				<span class="badge bg-info text-dark">New!!</span> Yet Another Simple Proof of the PCRP Theorem.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://conferences.au.dk/icalp2025">52nd EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2025)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICALP2025_SimplePCRP_abst" aria-expanded="false" aria-controls="ICALP2025_SimplePCRP_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
			</li>
			<div class="collapse" id="ICALP2025_SimplePCRP_abst">
				<p>
					The <i>Probabilistically Checkable Reconfiguration Proof</i> (PCRP) theorem, proven by Hirahara and Ohsaka (STOC 2024) and Karthik C. S. and Manurangsi, provides a new PCP-type characterization of <b>PSPACE</b>:
					A language $L$ is in <b>PSPACE</b> if and only if there exists a probabilistic verifier $\mathcal{V}$ and a pair of polynomial-time computable proofs $\pi^\mathsf{ini}, \pi^\mathsf{end}$ such that the following hold for every input $x$:
				<ul>
					<li>
						If $x \in L$, then $\pi^\mathsf{ini}(x)$ can be transformed into $\pi^\mathsf{end}(x)$ by repeatedly flipping a single bit of the proof at a time, while making $\mathcal{V}(x)$ to accept every intermediate proof with probability $1$.
					</li>
					<li>
						If $x \notin L$, then any such transformation induces a proof that is rejected by $\mathcal{V}(x)$ with probability more than $\frac{1}{2}$.
					</li>
				</ul>
				The PCRP theorem finds many applications in <b>PSPACE</b>-hardness of approximation for reconfiguration problems.
				</p>
				<p>
					In this paper, we present an alternative proof of the PCRP theorem that is “simpler” than those of Hirahara and Ohsaka and Karthik C. S. and Manurangsi.
					Our PCRP system is obtained by combining simple <i>robustization</i> and <i>composition</i> steps in a modular fashion, which renders its analysis more intuitive.
					The crux of implementing the robustization step is an error-correcting code that enjoys both <i>list decodability</i> and <i>reconfigurability</i>, the latter of which enables to reconfigure between a pair of codewords, while avoiding getting too close to any other codewords.
				</p>
			</div>


			<li>
				<span class="badge bg-info text-dark">New!!</span> Asymptotically Optimal Inapproximability of Maxmin $k$-Cut Reconfiguration.<br>
				<a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://conferences.au.dk/icalp2025">52nd EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2025)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICALP2025_kCutReconf_abst" aria-expanded="false" aria-controls="ICALP2025_kCutReconf_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2410.03416" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="ICALP2025_kCutReconf_abst">
				<p>
					<span class="small-caps">$k$-Coloring Reconfiguration</span> is one of the most well-studied reconfiguration problems, which asks to
					transform a given proper $k$-coloring of a graph to another by repeatedly recoloring a single vertex.
					Its approximate version, <span class="small-caps">Maxmin $k$-Cut Reconfiguration</span>,
					is defined as an optimization problem of maximizing the minimum fraction of bichromatic edges during the transformation between (not necessarily proper) $k$-colorings.
					In this paper,
					we demonstrate that the optimal approximation factor of this problem is $1 - \Theta\left(\frac{1}{k}\right)$ for every $k \geq 2$.
					Specifically, we prove the $\mathsf{PSPACE}$-hardness of approximating the objective value within a factor of $1 - \frac{\varepsilon}{k}$ for some universal constant $\varepsilon > 0$,
					whereas we develop a deterministic polynomial-time algorithm that achieves the approximation factor of $1 - \frac{2}{k}$.
				</p>
				<p>
					To prove the hardness result, we propose a new probabilistic verifier that tests a “striped” pattern.
					Our approximation algorithm is based on a random transformation that passes through a random $k$-coloring.
				</p>
			</div>


			<li>
				On Approximate Reconfigurability of Label Cover.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.journals.elsevier.com/information-processing-letters">Information Processing Letters</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#IPL2025_abst" aria-expanded="false" aria-controls="IPL2025_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1016/j.ipl.2024.106556" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2304.08746" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="IPL2025_abst">
				<p>
					Given a two-prover game $G$ and its two satisfying labelings $\psi_\mathsf{ini}$ and $\psi_\mathsf{tar}$,
					the <span class="small-caps">Label Cover Reconfiguration</span> problem asks whether $\psi_\mathsf{ini}$ can be transformed into $\psi_\mathsf{tar}$ by repeatedly changing the label of a single vertex while preserving any intermediate labeling satisfying $G$.
					We consider its optimization version by relaxing the feasibility of labelings, referred to as <span class="small-caps">Maxmin Label Cover Reconfiguration</span>:
					We are allowed to pass through any <i>non-satisfying</i> labelings, but required to maximize the “soundness error,” which is defined as the <i>minimum</i> fraction of satisfied edges during transformation from $\psi_\mathsf{ini}$ to $\psi_\mathsf{tar}$.
					Since the parallel repetition theorem of Raz (SIAM J. Comput., 1998), which implies <b>NP</b>-hardness of approximating <span class="small-caps">Label Cover</span> within any constant factor, gives strong inapproximability results for many <b>NP</b>-hard problems,
					one may think of using <span class="small-caps">Maxmin Label Cover Reconfiguration</span> to derive inapproximability results for reconfiguration problems.
					We prove the following results on <span class="small-caps">Maxmin Label Cover Reconfiguration</span>, which display different trends from those of <span class="small-caps">Label Cover</span> and the parallel repetition theorem:
				<ul>
					<li><span class="small-caps">Maxmin Label Cover Reconfiguration</span> can be approximated within a factor of $\frac{1}{4} - o(1)$ for some restricted graph classes, including biregular graphs, balanced bipartite graphs with no isolated vertices, and superconstant average degree graphs.</li>
					<li>
						A “naive” parallel repetition of <span class="small-caps">Maxmin Label Cover Reconfiguration</span> does not decrease the soundness error for <i>every</i> two-prover game.</li>
					<li><span class="small-caps">Label Cover Reconfiguration</span> on <i>projection games</i> can be decided in polynomial time.</li>
				</ul>
				The above results suggest that a reconfiguration analogue of the parallel repetition theorem is unlikely.
				</p>
			</div>


			<li>
				Matroid Semi-Bandits in Sublinear Time.<br>
				<a href="https://rctzeng.github.io/">Ruo-Chun Tzeng</a>,
				<u>Naoto Ohsaka</u>, and
				<a href="https://researchmap.jp/ariu">Kaito Ariu</a>.<br>
				<a href="https://icml.cc/Conferences/2024">40th International Conference on Machine Learning (ICML 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICML2024_abst" aria-expanded="false" aria-controls="ICML2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://openreview.net/forum?id=MwQ53xAIPs" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2405.17968" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="ICML2024_abst">
				<p>
					We study the matroid semi-bandits problem, where at each round the learner plays a subset of $K$ arms from a feasible set, and the goal is to maximize the expected cumulative linear rewards.
					Existing algorithms have per-round time complexity at least $\Omega(K)$, which becomes expensive when $K$ is large.
					To address this computational issue, we propose FasterCUCB whose sampling rule takes time sublinear in $K$ for common classes of matroids: $\mathcal{O}(D\,\mathrm{polylog}(K)\,\mathrm{polylog}(T))$ for uniform matroids, partition matroids, and graphical matroids, and $\mathcal{O}(D\sqrt{K}\mathrm{polylog}(T))$ for transversal matroids.
					Here, $D$ is the maximum number of elements in any feasible subset of arms, and $T$ is the horizon.
					Our technique is based on dynamic maintenance of an approximate maximum-weight basis over inner-product weights. Although the introduction of an approximate maximum-weight basis presents a challenge in regret analysis, we can still guarantee an upper bound on regret as tight as CUCB in the sense that it matches the gap-dependent lower bound by Kveton et al. (2014a) asymptotically.
				</p>
			</div>


			<li>
				Alphabet Reduction for Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://compose.ioc.ee/icalp2024/">51st EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICALP2024_ABCReductReconf_abst" aria-expanded="false" aria-controls="ICALP2024_ABCReductReconf_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.ICALP.2024.113" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2402.10627" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/ICALP2024_ABCReductReconf.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="ICALP2024_ABCReductReconf_abst">
				<p>
					We present a reconfiguration analogue of <i>alphabet reduction</i> à la Dinur (J. ACM, 2007) and its applications.
					Given a binary constraint graph $G$ and its two satisfying assignments $\psi^\mathsf{ini}$ and $\psi^\mathsf{tar}$, the $\textsf{Maxmin 2-CSP Reconfiguration}$ problem requests to transform $\psi^\mathsf{ini}$ into $\psi^\mathsf{tar}$ by repeatedly changing the value of a single vertex so that the minimum fraction of satisfied edges is maximized.
					We demonstrate a polynomial-time reduction from $\textsf{Maxmin 2-CSP Reconfiguration}$ with arbitrarily large alphabet size $W \in \mathbb{N}$ to itself with universal alphabet size $W_0 \in \mathbb{N}$ such that
				<ol>
					<li>the perfect completeness is preserved, and</li>
					<li>if any reconfiguration for the former violates $\varepsilon$-fraction of edges, then $\Omega(\varepsilon)$-fraction of edges must be unsatisfied during any reconfiguration for the latter.</li>
				</ol>
				The crux of its construction is the <i>reconfigurability of Hadamard codes</i>, which enables to reconfigure between a pair of codewords, while avoiding getting too close to the other codewords.
				Combining this alphabet reduction with gap amplification due to Ohsaka (SODA 2024), we are able to amplify the $1$ vs. $1-\varepsilon$ gap for arbitrarily small $\varepsilon \in (0,1)$ up to the $1$ vs. $1-\varepsilon_0$ for some universal $\varepsilon_0 \in (0,1)$ <i>without</i> blowing up the alphabet size.
				In particular, a $1$ vs. $1-\varepsilon_0$ gap version of $\textsf{Maxmin 2-CSP Reconfiguration}$ with alphabet size $W_0$ is <b>PSPACE</b>-hard given a probabilistically checkable reconfiguration proof system having any soundness error $1-\varepsilon$ due to Hirahara and Ohsaka (STOC 2024) and Karthik C. S. and Manurangsi (2023).
				As an immediate corollary, we show that under the same hypothesis, there exists a universal constant $\varepsilon_0 \in (0,1)$ such that many popular reconfiguration problems are <b>PSPACE</b>-hard to approximate within a factor of $1-\varepsilon_0$, including those of $\textsf{3-SAT}$, $\textsf{Independent Set}$, $\textsf{Vertex Cover}$, $\textsf{Clique}$, $\textsf{Dominating Set}$, and $\textsf{Set Cover}$.
				This may not be achieved only by gap amplification of Ohsaka, which makes the alphabet size gigantic depending on $\varepsilon^{-1}$.
				</p>
			</div>


			<li>
				Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration.<br>
				<a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://compose.ioc.ee/icalp2024/">51st EATCS International Colloquium on Automata, Languages, and Programming (ICALP 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICALP2024_SetCoverReconf_abst" aria-expanded="false" aria-controls="ICALP2024_SetCoverReconf_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.ICALP.2024.85" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2402.12645" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-primary" href="https://eccc.weizmann.ac.il/report/2024/039" role="button">
					<i class="bi bi-file-pdf-fill"></i> ECCC
				</a>
				<a class="btn btn-success" href="./slides/ICALP2024_SetCoverReconf.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="ICALP2024_SetCoverReconf_abst">
				<p>
					In the <span class="small-caps">Minmax Set Cover Reconfiguration</span> problem, given a set system $\mathcal{F}$ over a universe and its two covers $\mathcal{C}^\mathsf{start}$ and $\mathcal{C}^\mathsf{goal}$ of size $k$, we wish to transform $\mathcal{C}^\mathsf{start}$ into $\mathcal{C}^\mathsf{goal}$ by repeatedly adding or removing a single set of $\mathcal{F}$ while covering the universe in any intermediate state.
					Then, the objective is to minimize the maximize size of any intermediate cover during transformation.
					We prove that <span class="small-caps">Minmax Set Cover Reconfiguration</span> and <span class="small-caps">Minmax Dominating Set Reconfiguration</span> are $\mathsf{PSPACE}$-hard to approximate within a factor of $2-\frac{1}{\operatorname{polyloglog} N}$, where $N$ is the size of the universe and the number of vertices in a graph, respectively, improving upon Ohsaka (SODA 2024) and Karthik C. S. and Manurangsi (2023).
					This is the first result that exhibits a sharp threshold for the approximation factor of any reconfiguration problem because both problems admit a $2$-factor approximation algorithm as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (Theor. Comput. Sci., 2011).
					Our proof is based on a reconfiguration analogue of the FGLSS reduction from Probabilistically Checkable Reconfiguration Proofs of Hirahara and Ohsaka (2024).
					We also prove that for any constant $\varepsilon \in (0,1)$, <span class="small-caps">Minmax Hypergraph Vertex Cover Reconfiguration</span> on $\operatorname{poly}(\varepsilon^{-1})$-uniform hypergraphs is $\mathsf{PSPACE}$-hard to approximate within a factor of $2-\varepsilon$.
				</p>
			</div>


			<li>
				Computational Complexity of Normalizing Constants for the Product of Determinantal Point Processes.<br>
				Tatsuya Matsuoka and <u>Naoto Ohsaka</u>.<br>
				<a href="https://www.sciencedirect.com/journal/theoretical-computer-science">Theoretical Computer Science</a> (Full version of ICML 2020). <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#TCS2024_abst" aria-expanded="false" aria-controls="TCS2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1016/j.tcs.2024.114513" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2111.14148" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="TCS2024_abst">
				<p>
					We consider the product of determinantal point processes (DPPs), a point process whose probability mass is proportional to the product of principal minors of multiple matrices, as a natural, promising generalization of DPPs.
					We study the computational complexity of computing its normalizing constant, which is among the most essential probabilistic inference tasks.
					Our complexity-theoretic results (almost) rule out the existence of efficient algorithms for this task unless the input matrices are forced to have favorable structures.
					In particular, we prove the following:
				<ul>
					<li>
						Computing $\sum_S\det(\mathbf{A}_{S,S})^p$ exactly for every (fixed) positive even integer $p$ is $\textsf{UP}$-hard and $\textsf{Mod}_3\textsf{P}$-hard, which gives a negative answer to an open question posed by Kulesza and Taskar (2012).
					</li>
					<li>
						$\sum_S\det(\mathbf{A}_{S,S})\det(\mathbf{B}_{S,S})\det(\mathbf{C}_{S,S})$ is $\textsf{NP}$-hard to approximate within a factor of $2^{O(|\mathcal{I}|^{1-\varepsilon})}$ or $2^{\mathcal{O}(n^{1/\varepsilon})}$ for any $\varepsilon>0$, where $|\mathcal{I}|$ is the input size and $n$ is the order of the input matrix.
						This result is stronger than the #P-hardness for the case of two matrices derived by Gillenwater (2014).
					</li>
					<li>
						There exists a $k^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$-time algorithm for computing $\sum_S\det(\mathbf{A}_{S,S})\det(\mathbf{B}_{S,S})$, where $k$ is the maximum rank of $\mathbf{A}$ and $\mathbf{B}$ or the treewidth of the graph formed by nonzero entries of $\mathbf{A}$ and $\mathbf{B}$. Such parameterized algorithms are said to be fixed-parameter tractable.
					</li>
				</ul>
				These results can be extended to the fixed-size case.
				Further, we present two applications of fixed-parameter tractable algorithms given a matrix $\mathbf{A}$ of treewidth $w$:
				<ul>
					<li>
						We can compute a $2^{\frac{n}{2p-1}}$-approximation to $\sum_S\det({\bf A}_{S,S})^p$ for any fractional number $p>1$ in $w^{\mathcal{O}(wp)}n^{\mathcal{O}(1)}$ time.
					</li>
					<li>
						We can find a $2^{\sqrt n}$-approximation to unconstrained maximum a posteriori inference in $w^{\mathcal{O}(w\sqrt n)}n^{\mathcal{O}(1)}$ time.
					</li>
				</ul>
				</p>
			</div>


			<li>
				Probabilistically Checkable Reconfiguration Proofs and Inapproximability of Reconfiguration Problems.<br>
				<a href="https://researchmap.jp/shuichi.hirahara">Shuichi Hirahara</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="http://acm-stoc.org/stoc2024/">56th Annual ACM Symposium on Theory of Computing (STOC 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#STOC2024_abst" aria-expanded="false" aria-controls="STOC2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3618260.3649667" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2401.00474" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-primary" href="https://eccc.weizmann.ac.il/report/2024/023" role="button">
					<i class="bi bi-file-pdf-fill"></i> ECCC
				</a>
				<a class="btn btn-success" href="./slides/STOC2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<a class="btn btn-danger" href="https://www.youtube.com/watch?v=nBY5s2k4Zx0" role="button">
					<i class="bi bi-youtube"></i> Video
				</a>
			</li>
			<div class="collapse" id="STOC2024_abst">
				<p>
					Motivated by the inapproximability of reconfiguration problems, we present a new PCP-type characterization of $\mathsf{PSPACE}$, which we call a <i>probabilistically checkable reconfiguration proof</i> (PCRP):
					Any $\mathsf{PSPACE}$ computation can be encoded into an exponentially long sequence of polynomially long proofs such that every adjacent pair of the proofs differs in at most one bit, and every proof can be probabilistically checked by reading a constant number of bits.
				</p>
				<p>
					Using the new characterization, we prove $\mathsf{PSPACE}$-completeness of approximate versions of many reconfiguration problems, such as the <span class="small-caps">Maxmin 3-SAT Reconfiguration</span> problem.
					This resolves the open problem posed by Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (ISAAC 2008; Theor. Comput. Sci. 2011) as well as the Reconfiguration Inapproximability Hypothesis by Ohsaka (STACS 2023) affirmatively.
					We also present $\mathsf{PSPACE}$-completeness of approximating the <span class="small-caps">Maxmin Clique Reconfiguration</span> problem to within a factor of $n^\varepsilon$ for some constant $\varepsilon > 0$.
				</p>
			</div>


			<li>
				Safe Collaborative Filtering.<br>
				<a href="https://riktor.github.io/">Riku Togashi</a>,
				<a href="https://sites.google.com/site/homepageoka/tatsushi-oka">Tatsushi Oka</a>,
				<u>Naoto Ohsaka</u>, and
				Tetsuro Morimura.<br>
				<a href="https://iclr.cc/Conferences/2024">12th International Conference on Learning Representations (ICLR 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICLR2024_abst" aria-expanded="false" aria-controls="ICLR2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://openreview.net/forum?id=yarUvgEXq3" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2306.05292" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="ICLR2024_abst">
				<p>
					Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset.
					Tail performance is also a vital determinant of success for personalised recommender systems to reduce the risk of losing users with low satisfaction.
					This study introduces a "safe" collaborative filtering method that prioritises recommendation quality for less-satisfied users rather than focusing on the average performance.
					Our approach minimises the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss.
					To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS).
					Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maintaining competitive computational efficiency.
				</p>
			</div>


			<li>
				On the Parameterized Intractability of Determinant Maximization.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://link.springer.com/collections/gecffbehde">Algorithmica</a> (Special issue of ISAAC 2022). <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#Algorithmica2024_abst" aria-expanded="false" aria-controls="Algorithmica2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1007/s00453-023-01205-0" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2209.12519" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="Algorithmica2024_abst">
				<p>
					In the <span class="small-caps">Determinant Maximization</span> problem, given an $n \times n$ positive semi-definite matrix $\mathbf{A}$ in $\mathbb{Q}^{n \times n}$ and an integer $k$, we are required to find a $k \times k$ principal submatrix of $\mathbf{A}$ having the maximum determinant.
					This problem is known to be <b>NP</b>-hard and further proven to be <b>W[1]</b>-hard with respect to $k$ by Koutis; i.e., a $f(k)n^{\mathcal{O}(1)}$-time algorithm is unlikely to exist for any computable function $f$.
					However, there is still room to explore its parameterized complexity in the <i>restricted case</i>, in the hope of overcoming the general-case parameterized intractability.
					In this study, we rule out the fixed-parameter tractability of <span class="small-caps">Determinant Maximization</span> even if an input matrix is extremely sparse or low rank, or an approximate solution is acceptable.
					We first prove that <span class="small-caps">Determinant Maximization</span> is <b>NP</b>-hard and <b>W[1]</b>-hard even if an input matrix is an <i>arrowhead matrix</i>; i.e., the underlying graph formed by nonzero entries is a star, implying that the structural sparsity is not helpful.
					By contrast, we show that <span class="small-caps">Determinant Maximization</span> is solvable in polynomial time on <i>tridiagonal matrices</i>.
					Thereafter, we demonstrate the <b>W[1]</b>-hardness with respect to the <i>rank</i> $r$ of an input matrix.
					Our result is stronger than Koutis' result in the sense that any $k \times k$ principal submatrix is singular whenever $k > r$.
					We finally give evidence that it is <b>W[1]</b>-hard to approximate <span class="small-caps">Determinant Maximization</span> parameterized by $k$ within a factor of $2^{-c\sqrt{k}}$ for some universal constant $c > 0$.
					Our hardness result is conditional on the <i>Parameterized Inapproximability Hypothesis</i> posed by Lokshtanov, Ramanujan, Saurab, and Zehavi, which asserts that a gap version of Binary Constraint Satisfaction Problem is <b>W[1]</b>-hard.
					To complement this result, we develop an $\varepsilon$-additive approximation algorithm that runs in $\varepsilon^{-r^2} \cdot r^{\mathcal{O}(r^3)} \cdot n^{\mathcal{O}(1)}$ time for the rank $r$ of an input matrix, provided that the diagonal entries are bounded.
				</p>
			</div>


			<li>
				Gap Amplification for Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.siam.org/conferences/cm/conference/soda24">35th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2024)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SODA2024_abst" aria-expanded="false" aria-controls="SODA2024_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1137/1.9781611977912.54" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2310.14160" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/SODA2024.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="SODA2024_abst">
				<p>
					<i>Combinatorial reconfiguration</i> is an emerging field of theoretical computer science that studies the reachability between a pair of feasible solutions for a particular combinatorial problem.
					We study the hardness of accomplishing “approximate” reconfigurability, which affords to relax the feasibility of solutions.
					For example, in $\textsf{Minmax Set Cover Reconfiguration}$, given a pair of covers $\mathcal{C}_\mathsf{s}$ and $\mathcal{C}_\mathsf{t}$ for a set system $\mathcal{F}$, we aim to transform $\mathcal{C}_\mathsf{s}$ into $\mathcal{C}_\mathsf{t}$ by repeatedly adding or removing a single set of $\mathcal{F}$ so as to minimize the <i>maximum size</i> of covers during transformation.
					The recent study by Ohsaka (STACS 2023) gives evidence that a host of reconfiguration problems are <b>PSPACE</b>-hard to approximate assuming the <i>Reconfiguration Inapproximability Hypothesis</i> (RIH), which postulates that a gap version of $\textsf{Maxmin CSP Reconfiguration}$ is <b>PSPACE</b>-hard.
					One limitation of this approach is that inapproximability factors are not explicitly shown, so that even a $1.00 \cdots 001$-approximation algorithm for $\textsf{Minmax Set Cover Reconfiguration}$ may not be ruled out, whereas it admits $2$-approximation as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (Theor. Comput. Sci., 2011).
				</p>
				<p>
					In this paper, we demonstrate <i>gap amplification</i> for reconfiguration problems.
					In particular, we prove an explicit factor of <b>PSPACE</b>-hardness of approximation for three popular reconfiguration problems only assuming RIH.
					Our main result is that under RIH, $\textsf{Maxmin Binary CSP Reconfiguration}$ is <b>PSPACE</b>-hard to approximate within a factor of $0.9942$.
					Moreover, the same result holds even if the constraint graph is restricted to $(d,\lambda)$-expander for arbitrarily small $\frac{\lambda}{d}$.
					The crux of its proof is an alteration of the gap amplification technique due to Dinur (J. ACM, 2007), which amplifies the $1$ vs. $1-\varepsilon$ gap for arbitrarily small $\varepsilon > 0$ up to the $1$ vs. $1-0.0058$ gap.
					As an application of the main result, we demonstrate that $\textsf{Minmax Set Cover Reconfiguration}$ and $\textsf{Minmax Dominating Set Reconfiguration}$ are <b>PSPACE</b>-hard to approximate within a factor of $1.0029$ under RIH.
					Our proof is based on a gap-preserving reduction from Label Cover to Set Cover due to Lund and Yannakakis (J. ACM, 1994).
					However, unlike Lund--Yannakakis' reduction, the expander mixing lemma is essential to use.
					We highlight that all results hold unconditionally as long as “<b>PSPACE</b>-hard” is replaced by “<b>NP</b>-hard,” and are the first explicit inapproximability results for reconfiguration problems without resorting to the parallel repetition theorem.
					We finally complement the main result by showing that it is <b>NP</b>-hard to approximate $\textsf{Maxmin Binary CSP Reconfiguration}$ within a factor better than $\frac{3}{4}$.
				</p>
			</div>


			<li>
				Fast and Examination-agnostic Reciprocal Recommendation in Matching Markets.<br>
				<a href="https://miiitomi.github.io/">Yoji Tomita</a>,
				<a href="https://riktor.github.io/">Riku Togashi</a>,
				Yuriko Hashizume, and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://recsys.acm.org/recsys23">17th ACM Conference on Recommender Systems (RecSys 2023)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#RecSys2023_abst" aria-expanded="false" aria-controls="RecSys2023_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3604915.3608774" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2306.09060" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="RecSys2023_abst">
				<p>
					In matching markets such as job posting and online dating platforms, the recommender system plays a critical role in the success of the platform.
					Unlike standard recommender systems that suggest items to users, reciprocal recommender systems (RRSs) that suggest other users must take into account the mutual interests of users.
					In addition, ensuring that recommendation opportunities do not disproportionately favor popular users is essential for the total number of matches and for fairness among users.
					Existing recommendation methods in matching markets, however, face computational challenges on large-scale platforms and depend on specific examination functions in the position-based model (PBM).
					In this paper, we introduce the reciprocal recommendation method based on the matching with transferable utility (TU matching) model in the context of ranking recommendations in matching markets and propose a fast and examination-model-free algorithm.
					Furthermore, we evaluate our approach on experiments with synthetic data and real-world data from an online dating platform in Japan.
					Our method performs better than or as well as existing methods in terms of the total number of matches and works well even in a large-scale dataset for which one existing method does not work.
				</p>
			</div>


			<li>
				A Critical Reexamination of Intra-List Distance and Dispersion.<br>
				<u>Naoto Ohsaka</u> and
				<a href="https://riktor.github.io/">Riku Togashi</a>.
				<br>
				<a href="https://sigir.org/sigir2023">46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2023)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SIGIR2023_ILD_abst" aria-expanded="false" aria-controls="SIGIR2023_ILD_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3539618.3591623" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2305.13801" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/SIGIR2023.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="SIGIR2023_ILD_abst">
				<p>
					Diversification of recommendation results is a promising approach for coping with the uncertainty associated with users' information needs.
					Of particular importance in diversified recommendation is to define and optimize an appropriate diversity objective.
					In this study, we revisit the most popular diversity objective called <i>intra-list distance (ILD)</i>, defined as the average pairwise distance between selected items, and a similar but lesser known objective called <i>dispersion</i>, which is the minimum pairwise distance.
					Owing to their simplicity and flexibility, ILD and dispersion have been used in a plethora of diversified recommendation research.
					Nevertheless, <i>we do not actually know</i> what kind of items are preferred by them.
					We present a critical reexamination of ILD and dispersion from theoretical and experimental perspectives.
					Our theoretical results reveal that these objectives have potential drawbacks:
					ILD may select <i>duplicate</i> items that are very close to each other, whereas dispersion may overlook <i>distant</i> item pairs.
					As a competitor to ILD and dispersion, we design a diversity objective called Gaussian ILD, which can <i>interpolate</i> between ILD and dispersion by tuning the bandwidth parameter.
					We verify our theoretical results by experimental results using real-world data and confirm the extreme behavior of ILD and dispersion in practice.
				</p>
			</div>


			<li>
				Curse of “Low” Dimensionality in Recommender Systems.<br>
				<u>
					Naoto Ohsaka</u> and
				<a href="https://riktor.github.io/">Riku Togashi</a>
				(equal contribution).
				<br>
				<a href="https://sigir.org/sigir2023">46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2023)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SIGIR23_LowDim_abst" aria-expanded="false" aria-controls="SIGIR23_LowDim_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3539618.3591659" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2305.13597" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="SIGIR23_LowDim_abst">
				<p>
					Beyond accuracy, there are a variety of aspects to the quality of recommender systems, such as diversity, fairness, and robustness.
					We argue that many of the prevalent problems in recommender systems are partly due to <i>low-dimensionality</i> of user and item embeddings, particularly when dot-product models, such as matrix factorization, are used.
					In this study, we showcase empirical evidence suggesting the necessity of sufficient dimensionality for user/item embeddings to achieve diverse, fair, and robust recommendation.
					We then present theoretical analyses of the expressive power of dot-product models.
					Our theoretical results demonstrate that the number of possible rankings expressible under dot-product models is exponentially bounded by the dimension of item factors.
					We empirically found that the low-dimensionality contributes to a popularity bias, widening the gap between the rank positions of popular and long-tail items;
					we also give a theoretical justification for this phenomenon.
				</p>
			</div>


			<li>
				Gap Preserving Reductions Between Reconfiguration Problems.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.conferences.uni-hamburg.de/event/272/page/153-home">40th International Symposium on Theoretical Aspects of Computer Science (STACS 2023)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#STACS2023_abst" aria-expanded="false" aria-controls="STACS2023_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.STACS.2023.49" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2212.04207" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/STACS2023.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="STACS2023_abst">
				<p>
					<i>Combinatorial reconfiguration</i> is a growing research field studying problems on the transformability between a pair of solutions of a search problem.
					For example, in SAT Reconfiguration, for a Boolean formula $\varphi$ and two satisfying truth assignments $\sigma_\mathsf{s}$ and $\sigma_\mathsf{t}$ for $\varphi$, we are asked to determine whether there is a sequence of satisfying truth assignments for $\varphi$ starting from $\sigma_\mathsf{s}$ and ending with $\sigma_\mathsf{t}$, each resulting from the previous one by flipping a single variable assignment.
					We consider the approximability of <i>optimization variants</i> of reconfiguration problems; e.g., Maxmin SAT Reconfiguration requires to maximize the minimum fraction of satisfied clauses of $\varphi$ during transformation from $\sigma_\mathsf{s}$ to $\sigma_\mathsf{t}$.
					Solving such optimization variants approximately, we may be able to obtain a reasonable reconfiguration sequence comprising almost-satisfying truth assignments.
				</p>
				<p>
					In this study, we prove a series of <i>gap-preserving reductions</i> to give evidence that a host of reconfiguration problems are <b>PSPACE</b>-hard to approximate, under some plausible assumption.
					Our starting point is a new working hypothesis called the <i>Reconfiguration Inapproximability Hypothesis</i> (RIH), which asserts that a gap version of Maxmin CSP Reconfiguration is <b>PSPACE</b>-hard.
					This hypothesis may be thought of as a reconfiguration analogue of the PCP theorem.
					Our main result is <b>PSPACE</b>-hardness of approximating Maxmin $3$-SAT Reconfiguration of <i>bounded occurrence</i> under RIH.
					The crux of its proof is a gap-preserving reduction from Maxmin Binary CSP Reconfiguration to itself of <i>bounded degree</i>.
					Because a simple application of the degree reduction technique using expander graphs due to Papadimitriou and Yannakakis does not preserve the <i>perfect completeness</i>, we modify the alphabet as if each vertex could take a pair of values simultaneously.
					To accomplish the soundness requirement, we further apply an explicit family of near-Ramanujan graphs and the expander mixing lemma.
					As an application of the main result, we demonstrate that under RIH, optimization variants of popular reconfiguration problems are <b>PSPACE</b>-hard to approximate, including Nondeterministic Constraint Logic due to Hearn and Demaine, Independent Set Reconfiguration, Clique Reconfiguration, and Vertex Cover Reconfiguration.
				</p>
			</div>


			<li>
				On Reconfigurability of Target Sets.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.sciencedirect.com/journal/theoretical-computer-science">Theoretical Computer Science</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#TCS2022_abst" aria-expanded="false" aria-controls="TCS2022_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1016/j.tcs.2022.11.036" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2107.09885" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="TCS2022_abst">
				<p>
					We study the problem of deciding reconfigurability of target sets of a graph.
					Given a graph $G$ with vertex thresholds $\tau$, consider a dynamic process in which vertex $v$ becomes activated once at least $\tau(v)$ of its neighbors are activated.
					A vertex set $S$ is called a <i>target set</i> if all vertices of $G$ would be activated when initially activating vertices of $S$.
					In the Target Set Reconfiguration problem, given two target sets $X$ and $Y$ of the same size, we are required to determine whether $X$ can be transformed into $Y$ by repeatedly swapping one vertex in the current set with another vertex not in the current set preserving every intermediate set as a target set.
					In this paper, we investigate the complexity of Target Set Reconfiguration in restricted cases.
					On the hardness side, we prove that Target Set Reconfiguration is <b>PSPACE</b>-complete on bipartite planar graphs of degree $3$ and $4$ and of threshold $2$, bipartite $3$-regular graphs and planar $3$-regular graphs of threshold $1$ and $2$, and split graphs, which is in contrast to the fact that a special case called Vertex Cover Reconfiguration is in <b>P</b> for the last graph class.
					On the positive side, we present a polynomial-time algorithm for Target Set Reconfiguration on graphs of maximum degree $2$ and trees.
					The latter result can be thought of as a generalization of that for Vertex Cover Reconfiguration.
				</p>
			</div>


			<li>
				On the Parameterized Intractability of Determinant Maximization.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://isa.hanyang.ac.kr/isaac2022/">33rd International Symposium on Algorithms and Computation (ISAAC 2022)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ISAAC2022_abst" aria-expanded="false" aria-controls="ISAAC2022_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.ISAAC.2022.46" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2209.12519" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
				<a class="btn btn-success" href="./slides/ISAAC2022.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="ISAAC2022_abst">
				<p>
					In the Determinant Maximization problem, given an $n \times n$ positive semi-definite matrix $\mathbf{A}$ in $\mathbb{Q}^{n \times n}$ and an integer $k$, we are required to find a $k \times k$ principal submatrix of $\mathbf{A}$ having the maximum determinant.
					This problem is known to be <b>NP</b>-hard and further proven to be <b>W[1]</b>-hard with respect to $k$ by Koutis; i.e., a $f(k)n^{\mathcal{O}(1)}$-time algorithm is unlikely to exist for any computable function $f$.
					However, there is still room to explore its parameterized complexity in the <i>restricted case</i>, in the hope of overcoming the general-case parameterized intractability.
					In this study, we rule out the fixed-parameter tractability of Determinant Maximization even if an input matrix is extremely sparse or low rank, or an approximate solution is acceptable.
					We first prove that Determinant Maximization is <b>NP</b>-hard and <b>W[1]</b>-hard even if an input matrix is an <i>arrowhead matrix</i>; i.e., the underlying graph formed by nonzero entries is a star, implying that the structural sparsity is not helpful.
					By contrast, we show that Determinant Maximization is solvable in polynomial time on <i>tridiagonal matrices</i>.
					Thereafter, we demonstrate the <b>W[1]</b>-hardness with respect to the <i>rank</i> $r$ of an input matrix.
					Our result is stronger than Koutis' result in the sense that any $k \times k$ principal submatrix is singular whenever $k > r$.
					We finally give evidence that it is <b>W[1]</b>-hard to approximate Determinant Maximization parameterized by $k$ within a factor of $2^{-c\sqrt{k}}$ for some universal constant $c > 0$.
					Our hardness result is conditional on the <i>Parameterized Inapproximability Hypothesis</i> posed by Lokshtanov, Ramanujan, Saurab, and Zehavi, which asserts that a gap version of Binary Constraint Satisfaction Problem is <b>W[1]</b>-hard.
					To complement this result, we develop an $\varepsilon$-additive approximation algorithm that runs in $\varepsilon^{-r^2} \cdot r^{\mathcal{O}(r^3)} \cdot n^{\mathcal{O}(1)}$ time for the rank $r$ of an input matrix, provided that the diagonal entries are bounded.
				</p>
			</div>


			<li>
				Some Inapproximability Results of MAP Inference and Exponentiated Determinantal Point Processes.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.jair.org">Journal of Artificial Intelligence Research</a> (Full version of AISTATS 2021). <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#JAIR2022_abst" aria-expanded="false" aria-controls="JAIR2022_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1613/jair.1.13288" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2109.00727" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="JAIR2022_abst">
				<p>
					We study the computational complexity of two hard problems on determinantal point processes (DPPs).
					One is maximum a posteriori (MAP) inference, i.e., to find a principal submatrix having the maximum determinant.
					The other is probabilistic inference on exponentiated DPPs (E-DPPs), which can sharpen or weaken the diversity preference of DPPs with an exponent parameter $p$.
					We present several complexity-theoretic hardness results that explain the difficulty in approximating MAP inference and the normalizing constant for E-DPPs.
					We first prove that unconstrained MAP inference for an $n \times n$ matrix is <b>NP</b>-hard to approximate within a factor of $2^{\beta n}$, where $\beta = 10^{-10^{13}} $.
					This result improves upon the best-known inapproximability factor of $(\frac{9}{8}-\epsilon)$, and rules out the existence of any polynomial-factor approximation algorithm assuming <b>P</b> ≠ <b>NP</b>.
					We then show that log-determinant maximization is <b>NP</b>-hard to approximate within a factor of $\frac{5}{4}$ for the unconstrained case and within a factor of $1+10^{-10^{13}}$ for the size-constrained monotone case.
					In particular, log-determinant maximization does not admit a polynomial-time approximation scheme unless <b>P</b> = <b>NP</b>.
					As a corollary of the first result, we demonstrate that the normalizing constant for E-DPPs of any (fixed) constant exponent $p \geq \beta^{-1} = 10^{10^{13}}$ is <b>NP</b>-hard to approximate within a factor of $2^{\beta pn}$, which is in contrast to the case of $p \leq 1$ admitting a fully polynomial-time randomized approximation scheme.
				</p>
			</div>


			<li>
				Reconfiguration Problems on Submodular Functions.<br>
				<u>Naoto Ohsaka</u> and
				Tatsuya Matsuoka<br>
				<a href="https://www.wsdm-conference.org/2022/">15th ACM International Conference on Web Search and Data Mining (WSDM 2022)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#WSDM2022_abst" aria-expanded="false" aria-controls="WSDM2022_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3488560.3498382" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2111.14030" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="WSDM2022_abst">
				<p>
					Reconfiguration problems require finding a step-by-step transformation between a pair of feasible solutions for a particular problem.
					The primary concern in Theoretical Computer Science has been revealing their computational complexity for classical problems.
					<br>
					This paper presents an initial study on reconfiguration problems derived from a submodular function, which has more of a flavor of Data Mining.
					Our submodular reconfiguration problems request to find a solution sequence connecting two input solutions such that each solution has an objective value above a threshold in a submodular function $f: 2^{[n]} \to \mathbb{R}_+$ and is obtained from the previous one by applying a simple transformation rule.
					We formulate three reconfiguration problems:
					Monotone Submodular Reconfiguration (MSReco), which applies to influence maximization, and two versions of Unconstrained Submodular Reconfiguration (USReco), which apply to determinantal point processes.
					Our contributions are summarized as follows:
				<ul>
					<li>We prove that MSReco and USReco are both <b>PSPACE</b>-complete.</li>
					<li>We design a $\frac{1}{2}$-approximation algorithm for MSReco and a $\frac{1}{n}$-approximation algorithm for (one version of) USReco.</li>
					<li>We devise inapproximability results that approximating the optimum value of MSReco within a $(1-\frac{1+\epsilon}{n^2})$-factor is <b>PSPACE</b>-hard, and we cannot find a $(\frac{5}{6}+\epsilon)$-approximation for USReco.</li>
					<li>We conduct numerical study on the reconfiguration version of influence maximization and determinantal point processes using real-world social network and movie rating data.</li>
				</ul>
				</p>
			</div>


			<li>
				On the Convex Combination of Determinantal Point Processes.<br>
				Tatsuya Matsuoka,
				<u>Naoto Ohsaka</u>, and
				Akihiro Yabe.<br>
				<a href="http://www.acml-conf.org/2021/">13th Asian Conference on Machine Learning (ACML 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ACML2021_DPP_abst" aria-expanded="false" aria-controls="ACML2021_DPP_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://proceedings.mlr.press/v157/matsuoka21a.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="ACML2021_DPP_abst">
				<p>
					Determinantal point processes (DPPs) are attractive probabilistic models for expressing item quality and set diversity simultaneously.
					Although DPPs are widely-applicable to many subset selection tasks, there exist simple small-size probability distributions that any DPP cannot express.
					To overcome this drawback while keeping good properties of DPPs, in this paper we investigate the expressive power of convex combinations of DPPs.
					We provide upper and lower bounds for the number of DPPs required for exactly expressing any probability distribution.
					For the approximation error, we give an upper bound on the Kullback--Leibler divergence $n-\lfloor \log t\rfloor +\epsilon$ for any $\epsilon >0$ of approximate distribution from a given joint probability distribution, where $t$ is the number of DPPs.
					Our numerical simulation on an online retail dataset empirically verifies that a convex combination of only two DPPs can outperform a nonsymmetric DPP in terms of the Kullback--Leibler divergence.
					By combining a polynomial number of DPPs, we can express probability distributions induced by bounded-degree pseudo-Boolean functions, which include weighted coverage functions of bounded occurrence.
				</p>
			</div>


			<li>
				Maximization of Monotone $k$-Submodular Functions with Bounded Curvature and Non-$k$-Submodular Functions.<br>
				Tatsuya Matsuoka and
				<u>Naoto Ohsaka</u>.<br>
				<a href="http://www.acml-conf.org/2021/">13th Asian Conference on Machine Learning (ACML 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ACML2021_KSFM_abst" aria-expanded="false" aria-controls="ACML2021_KSFM_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://proceedings.mlr.press/v157/matsuoka21b.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="ACML2021_KSFM_abst">
				<p>
					The concept of $k$-submodularity is an extension of submodularity, of which maximization has various applications, such as influence maximization and sensor placement.
					In such situations, to model complicated real problems, we want to deal with multiple factors, such as, more detailed parameter representing a property of a given function or a constraint which should be imposed for a given function, simultaneously.
					Besides, it is preferable that an algorithm for the modeling problem is simple. In this paper, for both monotone $k$-submodular function maximization with bounded curvature and monotone weakly $k$-submodular function maximization, we give approximation ratio analysis on greedy-type algorithms on the problem with the matroid constraint and that with the individual size constraint.
					Furthermore, we give an approximation ratio analysis on another type of the relaxation of $k$-submodular functions, approximately $k$-submodular functions, with the matroid constraint.
				</p>
			</div>

			<li>
				Approximation Algorithm for Submodular Maximization under Submodular Cover.<br>
				<u>Naoto Ohsaka</u> and
				Tatsuya Matsuoka.<br>
				<a href="https://www.auai.org/uai2021/">37th Conference on Uncertainty in Artificial Intelligence (UAI 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#UAI2021_abst" aria-expanded="false" aria-controls="UAI2021_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://proceedings.mlr.press/v161/ohsaka21a.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="UAI2021_abst">
				<p>
					We study a new optimization problem called submodular maximization under submodular cover (SMSC), which requires to find a fixed-size set such that one monotone submodular function $f$ is maximized subject to that another monotone submodular function $g$ is maximized approximately.
					SMSC is preferable to submodular function maximization when one wants to maximize two objective functions simultaneously.
					We propose an optimization framework for SMSC, which guarantees a constant-factor approximation.
					Our algorithm's key idea is to construct a new instance of submodular function maximization from a given instance of SMSC, which can be approximated efficiently.
					Besides, if we are given an approximation oracle for submodular function maximization, our algorithm provably produces nearly optimal solutions.
					We experimentally evaluate the proposed algorithm in terms of sensor placement and movie recommendation using real-world data.
				</p>
			</div>

			<li>
				A Fully Polynomial Parameterized Algorithm for Counting the Number of Reachable Vertices in a Digraph.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.journals.elsevier.com/information-processing-letters">Information Processing Letters</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#IPL2021_abst" aria-expanded="false" aria-controls="IPL2021_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1016/j.ipl.2021.106137" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2103.04595" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="IPL2021_abst">
				<p>
					We consider the problem of counting the number of vertices reachable from each vertex in a digraph $G$, which is equal to computing all the out-degrees of the transitive closure of $G$.
					The current (theoretically) fastest algorithms run in quadratic time; however, Borassi has shown that this problem is not solvable in truly subquadratic time unless the Strong Exponential Time Hypothesis fails [Inf. Process. Lett., 116(10):628--630, 2016].
					In this paper, we present an $\mathcal{O}(f^3n)$-time exact algorithm, where $n$ is the number of vertices in $G$ and $f$ is the feedback edge number of $G$.
					Our algorithm thus runs in truly subquadratic time for digraphs of $f=\mathcal{O}(n^{\frac{1}{3}-\epsilon})$ for any $\epsilon > 0$, i.e., the number of edges is $n$ plus $\mathcal{O}(n^{\frac{1}{3}-\epsilon})$, and is fully polynomial fixed parameter tractable, the notion of which was first introduced by Fomin, Lokshtanov, Pilipczuk, Saurabh, and Wrochna [ACM Trans. Algorithms, 14(3):34:1--34:45, 2018].
					We also show that the same result holds for vertex-weighted digraphs, where the task is to compute the total weights of vertices reachable from each vertex.
				</p>
			</div>

			<li>
				Spanning Tree Constrained Determinantal Point Processes are Hard to (Approximately) Evaluate.<br>
				Tatsuya Matsuoka and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.journals.elsevier.com/operations-research-letters">Operations Research Letters</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ORL2021_abst" aria-expanded="false" aria-controls="ORL2021_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1016/j.orl.2021.02.004" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2102.12646" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="ORL2021_abst">
				<p>
					We consider determinantal point processes (DPPs) constrained by spanning trees.
					Given a graph $G=(V,E)$ and a positive semi-definite matrix $\mathbf{A}$ indexed by $E$, a spanning-tree DPP defines a distribution such that we draw $S\subseteq E$ with probability proportional to $\det(\mathbf{A}_S)$ only if $S$ induces a spanning tree.
					We prove $\sharp\textsf{P}$-hardness of computing the normalizing constant for spanning-tree DPPs and provide an approximation-preserving reduction from the mixed discriminant, for which FPRAS is not known.
					We show similar results for DPPs constrained by forests.
				</p>
			</div>

			<li>
				Unconstrained MAP Inference, Exponentiated Determinantal Point Processes, and Exponential Inapproximability.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://aistats.org/aistats2021">24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#AISTATS2021_DPP_abst" aria-expanded="false" aria-controls="AISTATS2021_DPP_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="http://proceedings.mlr.press/v130/ohsaka21a.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="AISTATS2021_DPP_abst">
				<p>
					We study the computational complexity of two hard problems on determinantal point processes (DPPs).
					One is maximum a posteriori (MAP) inference, i.e., to find a principal submatrix having the maximum determinant.
					The other is probabilistic inference on exponentiated DPPs (E-DPPs), which can sharpen or weaken the diversity preference of DPPs with an exponent parameter $p$.
					We prove the following complexity-theoretic hardness results that explain the difficulty in approximating unconstrained MAP inference and the normalizing constant for E-DPPs.
				<ul>
					<li>
						Unconstrained MAP inference for an $n \times n$ matrix is $\textsf{NP}$-hard to approximate within a $2^{\beta n}$-factor, where $\beta = 10^{-10^{13}} $.
						This result improves upon a $(\frac{9}{8}-\epsilon)$-factor inapproximability given by Kulesza and Taskar (2012).
					</li>
					<li>
						The normalizing constant for E-DPPs of any (fixed) constant exponent $p \geq \beta^{-1} = 10^{10^{13}}$ is $\textsf{NP}$-hard to approximate within a $2^{\beta pn}$-factor.
						This gives a(nother) negative answer to open questions posed by Kulesza andTaskar (2012); Ohsaka and Matsuoka (2020).
					</li>
				</ul>
				</p>
			</div>

			<li>
				Tracking Regret Bounds for Online Submodular Optimization.<br>
				Tatsuya Matsuoka,
				<a href="https://researchmap.jp/shinji_ito">Shinji Ito</a>, and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://aistats.org/aistats2021">24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#AISTATS2021_OSO_abst" aria-expanded="false" aria-controls="AISTATS2021_OSO_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="http://proceedings.mlr.press/v130/matsuoka21a.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="AISTATS2021_OSO_abst">
				<p>
					In this paper, we propose algorithms for online submodular optimization with tracking regret bounds.
					Online submodular optimization is a generic framework for sequential decision making used to select subsets.
					Existing algorithms for online submodular optimization have been shown to achieve small (static) regret, which means that the algorithm’s performance is comparable to the performance of a fixed optimal action.
					Such algorithms, however, may perform poorly in an environment that changes over time.
					To overcome this problem, we apply a tracking-regret-analysis framework to online submodular optimization, one by which output is assessed through comparison with time-varying optimal subsets.
					We propose algorithms for submodular minimization, monotone submodular maximization under a size constraint, and unconstrained submodular maximization, and we show tracking regret bounds.
					In addition, we show that our tracking regret bound for submodular minimization is nearly tight.
				</p>
			</div>


			<li>
				Predictive Optimization with Zero-Shot Domain Adaptation.<br>
				<a href="https://t-sakai-kure.github.io/">Tomoya Sakai</a> and
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://www.siam.org/conferences/cm/conference/sdm21">2021 SIAM International Conference on Data Mining (SDM 2021)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SDM2021_abst" aria-expanded="false" aria-controls="SDM2021_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1137/1.9781611976700.42" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2101.06233" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="SDM2021_abst">
				<p>
					Prediction in a new domain without any training sample, called zero-shot domain adaptation (ZSDA), is an important task in domain adaptation.
					While prediction in a new domain has gained much attention in recent years, in this paper, we investigate another potential of ZSDA.
					Specifically, instead of predicting responses in a new domain, we find a description of a new domain given a prediction.
					The task is regarded as predictive optimization, but existing predictive optimization methods have not been extended to handling multiple domains.
					We propose a simple framework for predictive optimization with ZSDA and analyze the condition in which the optimization problem becomes convex optimization.
					We also discuss how to handle the interaction of characteristics of a domain in predictive optimization.
					Through numerical experiments, we demonstrate the potential usefulness of our proposed framework.
				</p>
			</div>


			<li>
				On the (In)tractability of Computing Normalizing Constants for the Product of Determinantal Point Processes.<br>
				<u>Naoto Ohsaka</u> and
				Tatsuya Matsuoka.<br>
				<a href="https://icml.cc/Conferences/2020/">37th International Conference on Machine Learning (ICML 2020)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#ICML2020_abst" aria-expanded="false" aria-controls="ICML2020_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="http://proceedings.mlr.press/v119/ohsaka20a.html" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-danger" href="https://icml.cc/virtual/2020/poster/6379" role="button">
					<i class="bi bi-youtube"></i> Video
				</a>
			</li>
			<div class="collapse" id="ICML2020_abst">
				<p>
					We consider the product of determinantal point processes (DPPs), a point process whose probability mass is proportional to the product of principal minors of multiple matrices as a natural, promising generalization of DPPs.
					We study the computational complexity of computing its normalizing constant, which is among the most essential probabilistic inference tasks.
					Our complexity-theoretic results (almost) rule out the existence of efficient algorithms for this task, unless input matrices are forced to have favorable structures.
					In particular, we prove the following:
				<ul>
					<li>
						Computing $ \sum_{S} \det(\mathbf{A}_{S,S})^p $ exactly for every (fixed) positive even integer $p$ is $\textsf{UP}$-hard and $\textsf{Mod}_3\textsf{P}$-hard,
						which gives a negative answer to an open question posed by Kulesza and Tasker (2012).
					</li>
					<li>
						$ \sum_{S} \det(\mathbf{A}_{S,S}) \det(\mathbf{B}_{S,S}) \det(\mathbf{C}_{S,S}) $
						is $\textsf{NP}$-hard to approximate within a factor of $ 2^{\mathcal{O}(|\mathcal{I}|^{1-\epsilon})} $ for any $\epsilon > 0$, where $|\mathcal{I}|$ is the input size.
						This result is stronger than $\sharp\textsf{P}$-hardness for the case of two matrices by Gillenwater (2014).
					</li>
					<li>
						There exists a $ k^{\mathcal{O}(k)} |\mathcal{I}|^{\mathcal{O}(1)} $-time algorithm for
						computing $\sum_{S} \det(\mathbf{A}_{S,S}) \det(\mathbf{B}_{S,S})$, where
						$k$ is ``the maximum rank of $\mathbf{A}$ and $\mathbf{B}$'' or
						``the treewidth of the graph induced by nonzero entries of $\mathbf{A}$ and $\mathbf{B}$.''
						Such parameterized algorithms are said to be fixed-parameter tractable.
					</li>
				</ul>
				</p>
			</div>


			<li>
				The Solution Distribution of Influence Maximization: A High-level Experimental Study on Three Algorithmic Approaches.<br>
				<u>Naoto Ohsaka</u>.<br>
				<a href="https://sigmod2020.org/">ACM SIGMOD International Conference on Management of Data 2020 (SIGMOD 2020)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SIGMOD2020_abst" aria-expanded="false" aria-controls="SIGMOD2020_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3318464.3380564" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2003.09816" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="SIGMOD2020_abst">
				<p>
					Influence maximization is among the most fundamental algorithmic problems in social influence analysis.
					Over the last decade, a great effort has been devoted to developing efficient algorithms for influence maximization, so that identifying the ``best'' algorithm has become a demanding task.
					In SIGMOD'17, Arora, Galhotra, and Ranu reported benchmark results on eleven existing algorithms and demonstrated that there is no single state-of-the-art offering the best trade-off between computational efficiency and solution quality.
				</p>
				<p>
					In this paper, we report a high-level experimental study on three well-established algorithmic approaches for influence maximization, referred to as Oneshot, Snapshot, and Reverse Influence Sampling (RIS).
					Different from Arora et al., our experimental methodology is so designed that we examine the distribution of random solutions, characterize the relation between the sample number and the actual solution quality, and avoid implementation dependencies.
					Our main findings are as follows:
					1. For a sufficiently large sample number, we obtain a unique solution regardless of algorithms.
					2. The average solution quality of Oneshot, Snapshot, and RIS improves at the same rate up to scaling of sample number.
					3. Oneshot requires more samples than Snapshot, and Snapshot requires fewer but larger samples than RIS.
					We discuss the time efficiency when conditioning Oneshot, Snapshot, and RIS to be of identical accuracy.
					Our conclusion is that Oneshot is suitable only if the size of available memory is limited, and RIS is more efficient than Snapshot for large networks; Snapshot is preferable for small, low-probability networks.
				</p>
			</div>

			<li>
				A Predictive Optimization Framework for Hierarchical Demand Matching.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://t-sakai-kure.github.io/">Tomoya Sakai</a>, and
				Akihiro Yabe.<br>
				<a href="https://www.siam.org/conferences/cm/conference/sdm20">2020 SIAM International Conference on Data Mining (SDM 2020)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SDM2020_abst" aria-expanded="false" aria-controls="SDM2020_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1137/1.9781611976236.20" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
			</li>
			<div class="collapse" id="SDM2020_abst">
				<p>
					Predictive optimization is a framework for designing an entire data-analysis pipeline that comprises both prediction and optimization, to be able to maximize overall throughput performance.
					In practical demand analysis, a knowledge of hierarchies, which might be geographical or categorical, is recognized as useful, though such additional knowledge has not been taken into account in existing predictive optimization.
					In this paper, we propose a novel hierarchical predictive optimization pipeline that is able to deal with a wide range of applications including inventory management.
					Based on an existing hierarchical demand prediction model, we present a stochastic matching framework that can manage prediction-uncertainty in decision making.
					We further provide a greedy approximation algorithm for solving demand matching on hierarchical structures.
					In experimental evaluations on both artificial and real-world data, we demonstrate the effectiveness of our proposed hierarchical-predictive-optimization pipeline.
				</p>
			</div>

			<li>
				Boosting PageRank Scores by Optimizing Internal Link Structure.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://researchmap.jp/t-sonobe/">Tomohiro Sonobe</a>,
				<a href="http://www.math.keio.ac.jp/~kakimura/index.html">Naonori Kakimura</a>,
				<a href="https://researchmap.jp/takuro_fukunaga/">Takuro Fukunaga</a>,
				Sumio Fujita, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://www.dexa.org/dexa2018">29th International Conference on Database and Expert Systems Applications (DEXA 2018)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#DEXA2018_abst" aria-expanded="false" aria-controls="DEXA2018_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1007/978-3-319-98809-2_26" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/DEXA2018.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="DEXA2018_abst">
				<p>
					We consider and formulate problems of PageRank score boosting motivated by applications such as effective web advertising.
					More precisely, given a graph and target vertices, one is required to find a fixed-size set of missing edges that maximizes the minimum PageRank score among the targets.
					We provide theoretical analyses to show that all of them are NP-hard.
					To overcome the hardness, we develop heuristic-based algorithms for them.
					We finally perform experiments on several real-world networks to verify the effectiveness of the proposed algorithms compared to baselines.
					Specifically, our algorithm achieves 100 times improvements of the minimum PageRank score among selected 100 vertices by adding only dozens of edges.
				</p>
			</div>

			<li>
				NoSingles: A Space-Efficient Algorithm for Influence Maximization.<br>
				Diana Popova,
				<u>Naoto Ohsaka,</u>
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi,</a> and
				<a href="http://webhome.cs.uvic.ca/~thomo/">Alex Thomo.</a><br>
				<a href="http://ssdbm2018.inf.unibz.it/">30th International Conference on Scientific and Statistical Database Management (SSDBM 2018)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SSDBM2018_abst" aria-expanded="false" aria-controls="SSDBM2018_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3221269.3221291" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
			</li>
			<div class="collapse" id="SSDBM2018_abst">
				<p>
					Algorithmic problems of computing influence estimation and influence maximization have been actively researched for decades.
					We developed a novel algorithm, NoSingles, based on the Reverse Influence Sampling method proposed by Borgs et al. in 2013.
					NoSingles solves the problem of influence maximization in large graphs using much smaller space than the existing state-of-the-art algorithms while preserving the theoretical guarantee of the approximation of $(1-1/e-\epsilon)$ of the optimum, for any $ \epsilon > 0 $.
					The NoSingles data structure is saved on the hard drive of the machine, and can be used repeatedly for playing out ``what if'' scenarios (e.g. trying different combination of seeds and calculating the influence spread).
					We also introduce a variation of NoSingles algorithm, which further decreases the running time, while preserving the approximation guarantee.
					We support our claims with extensive experiments on large real-world graphs.
					Savings in required space allow to successfully run NoSingles on a consumer-grade laptop for graphs with tens of millions of vertices and hundreds of millions of edges.
				</p>
			</div>

			<li>
				On the Power of Tree-Depth for Fully Polynomial FPT Algorithms.<br>
				<a href="http://research.nii.ac.jp/~yiwata/">Yoichi Iwata</a>,
				Tomoaki Ogasawara, and
				<u>Naoto Ohsaka.</u><br>
				<a href="https://stacs2018.sciencesconf.org/">35th International Symposium on Theoretical Aspects of Computer Science (STACS 2018)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#STACS2018_abst" aria-expanded="false" aria-controls="STACS2018_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.4230/LIPIcs.STACS.2018.41" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-primary" href="https://arxiv.org/abs/1710.04376" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="STACS2018_abst">
				<p>
					There are many classical problems in P whose time complexities have not been improved over the past decades.
					Recent studies of ``Hardness in P'' have revealed that, for several of such problems, the current fastest algorithm is the best possible under some complexity assumptions.
					To bypass this difficulty, Fomin et al. (SODA 2017) introduced the concept of fully polynomial FPT algorithms.
					For a problem with the current best time complexity $O(n^c)$, the goal is to design an algorithm running in $k^{O(1)}n^{c'}$ time for a parameter $k$ and a constant $c' < c$. <br>
						In this paper, we investigate the complexity of graph problems in P parameterized by tree-depth, a graph parameter related to tree-width.
						We show that a simple divide-and-conquer method can solve many graph problems, including Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle, Replacement Paths, and 2-hop Cover, in $O(\mathrm{td}\cdot m)$ time or $O(\mathrm{td}\cdot (m+n\log n))$ time, where $\mathrm{td}$ is the tree-depth of the input graph.
						Because any graph of tree-width $\mathrm{tw}$ has tree-depth at most $(\mathrm{tw}+1)\log_2 n$, our algorithms also run in $O(\mathrm{tw}\cdot m\log n)$ time or $O(\mathrm{tw}\cdot (m+n\log n)\log n)$ time.
						These results match or improve the previous best algorithms parameterized by tree-width.
						Especially, we solve an open problem of fully polynomial FPT algorithm for Weighted Matching parameterized by tree-width posed by Fomin et al.
				</p>
			</div>


			<li>
				Coarsening Massive Influence Networks for Scalable Diffusion Analysis.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://researchmap.jp/t-sonobe/">Tomohiro Sonobe</a>,
				Sumio Fujita, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://sigmod2017.org/">2017 ACM SIGMOD International Conference on Management of Data 2017 (SIGMOD 2017)</a>. <br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#SIGMOD2017_abst" aria-expanded="false" aria-controls="SIGMOD2017_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3035918.3064045" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/SIGMOD2017.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<a class="btn btn-danger" href="https://github.com/chronotable/coarsening-infnet" role="button">
					<i class="bi bi-github"></i> Code
				</a>
			</li>
			<div class="collapse" id="SIGMOD2017_abst">
				<p>
					Fueled by the increasing popularity of online social networks, social influence analysis has attracted a great deal of research attention in the past decade.
					The diffusion process is often modeled using influence graphs, and there has been a line of research that involves algorithmic problems in influence graphs.
					However, the vast size of today's real-world networks raises a serious issue with regard to computational efficiency.
				</p>
				<p>
					In this paper, we propose a new algorithm for reducing influence graphs.
					Given an input influence graph, the proposed algorithm produces a vertex-weighted influence graph, which is compact and approximates the diffusion properties of the input graph.
					The central strategy of influence graph reduction is coarsening, which has the potential to greatly reduce the number of edges by merging a vertex set into a single weighted vertex.
					We provide two implementations; a speed-oriented implementation which runs in linear time with linear space and a scalability-oriented implementation which runs in practically linear time with sublinear space.
					Further, we present general frameworks using our compact graphs that accelerate existing algorithms for influence maximization and influence estimation problems, which are motivated by practical applications, such as viral marketing.
					Using these frameworks, we can quickly obtain solutions that have accuracy guarantees under a reasonable assumption.
					Experiments with real-world networks demonstrate that the proposed algorithm can scale to billion-edge graphs and reduce the graph size to up to 4%.
					In addition, our influence maximization framework achieves four times speed-up of a state-of-the-art D-SSA algorithm, and
					our influence estimation framework cuts down the computation time of a simulation-based method to 3.5%.
				</p>
			</div>


			<li>
				Portfolio Optimization for Influence Spread.<br>
				<u>Naoto Ohsaka</u> and
				<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida.</a><br>
				<a href="http://www.www2017.com.au/">26th International Conference on World Wide Web (WWW 2017)</a>.<br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#WWW2017_abst" aria-expanded="false" aria-controls="WWW2017_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://doi.org/10.1145/3038912.3052628" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/WWW2017.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>
			<div class="collapse" id="WWW2017_abst">
				<p>
					Motivated by viral marketing, stochastic diffusion processes that model influence spread on a network have been studied intensively.
					The primary interest in such models has been to find a seed set of a fixed size that maximizes the expected size of the cascade from it.
					Practically, however, it is not desirable to have the risk of ending with a small cascade, even if the expected size of the cascade is large.
					To address this issue, we adopt conditional value at risk (CVaR) as a risk measure, and propose an algorithm that computes a portfolio over seed sets with a provable guarantee on its CVaR.
					Using real-world social networks, we demonstrate that the portfolio computed by our algorithm has a significantly better CVaR than seed sets computed by other baseline methods.
				</p>
			</div>
			</li>


			<li>
				Maximizing Time-Decaying Influence in Social Networks.<br>
				<u>Naoto Ohsaka</u>,
				<a href="http://www-sys.ist.osaka-u.ac.jp/~ymgc/">Yutaro Yamaguchi</a>,
				<a href="http://www.math.keio.ac.jp/~kakimura/index.html">Naonori Kakimura</a>, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://www.ecmlpkdd2016.org/">15th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2016)</a>.<br>
				<a class="btn btn-primary" href="https://doi.org/10.1007/978-3-319-46128-1_9" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/ECMLPKDD2016.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>


			<li>
				Dynamic Influence Analysis in Evolving Networks.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://takiba.net/">Takuya Akiba</a>,
				<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a>, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://www.vldb.org/pvldb/vol9.html">Proceedings of the VLDB Endowment (PVLDB 16)</a>.<br>
				<a class="btn btn-primary" href="http://www.vldb.org/pvldb/vol9/p1077-ohsaka.pdf" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/VLDB2016.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<a class="btn btn-danger" href="https://github.com/todo314/dynamic-influence-analysis" role="button">
					<i class="bi bi-github"></i> Code
				</a>
			</li>


			<li>
				Monotone $k$-Submodular Function Maximization with Size Constraints.<br>
				<u>Naoto Ohsaka</u> and
				<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a><br>
				<a href="https://nips.cc/Conferences/2015/">29th Annual Conference on Neural Information Processing Systems (NIPS 2015)</a>.<br>
				<a class="btn btn-primary" href="http://papers.nips.cc/paper/5709-monotone-k-submodular-function-maximization-with-size-constraints" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/NIPS2015.pdf" role="button">
					<i class="bi bi-file-pdf-fill"></i> Poster
				</a>
			</li>


			<li>
				Efficient PageRank Tracking in Evolving Networks.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://tmaehara.gitlab.io/">Takanori Maehara</a>, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://www.kdd.org/kdd2015/">21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2015)</a>.<br>
				<a class="btn btn-primary" href="https://doi.org/10.1145/2783258.2783297" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/KDD2015.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>


			<li>
				Fast and Accurate Influence Maximization on Large Networks with Pruned Monte-Carlo Simulations.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://takiba.net/">Takuya Akiba</a>,
				<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a>, and
				<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
				<a href="http://www.aaai.org/Conferences/AAAI/aaai14.php">28th AAAI Conference on Artificial Intelligence (AAAI 2014)</a>.<br>
				<a class="btn btn-primary" href="https://doi.org/10.1609/aaai.v28i1.8726" role="button">
					<i class="ai ai-open-access ai-lg"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/AAAI2014.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
				<a class="btn btn-danger" href="https://github.com/todo314/pruned-monte-carlo" role="button">
					<i class="bi bi-github"></i> Code
				</a>
			</li>

			<li>
				A Reinforcement Learning Method to Improve the Sweeping Efficiency for an Agent.<br>
				<u>Naoto Ohsaka</u>,
				<a href="https://nittc.tokyo-ct.ac.jp/web/j/usr/kitakosi/index-e.html">Daisuke Kitakoshi</a>, and
				<a href="https://researchmap.jp/MasatoSuzuki">Masato Suzuki</a>.<br>
				<a href="https://ieeexplore.ieee.org/xpl/conhome/6112290/proceeding">2011 IEEE International Conference on Granular Computing (GrC 2011)</a>.<br>
				<a class="btn btn-primary" href="https://doi.org/10.1109/GRC.2011.6122650" role="button">
					<i class="bi bi-file-pdf-fill"></i> Paper
				</a>
				<a class="btn btn-success" href="./slides/GrC2011.pdf" role="button">
					<i class="bi bi-file-slides-fill"></i> Slides
				</a>
			</li>

		</ol>

		<hr>






		<h3>Preprints</h3>

		<ol reversed>

			<li>
				Tight Inapproximability of Target Set Reconfiguration.<br>
				<u>Naoto Ohsaka</u>.<br>
				<button type="button" class="btn btn-warning" data-bs-toggle="collapse" data-bs-target="#TSReconfApprox_abst" aria-expanded="false" aria-controls="TSReconfApprox_abst">
					<i class="bi bi-chevron-expand"></i> Abstract
				</button>
				<a class="btn btn-primary" href="https://arxiv.org/abs/2402.15076" role="button">
					<i class="ai ai-arxiv"></i> arXiv
				</a>
			</li>
			<div class="collapse" id="TSReconfApprox_abst">
				<p>
					Given a graph $G$ with a vertex threshold function $\tau$, consider a dynamic process in which any inactive vertex $v$ becomes activated whenever at least $\tau(v)$ of its neighbors are activated.
					A vertex set $S$ is called a <i>target set</i> if all vertices of $G$ would be activated when initially activating vertices of $S$.
					In the $\textsf{Minmax Target Set Reconfiguration}$ problem, for a graph $G$ and its two target sets $X$ and $Y$, we wish to transform $X$ into $Y$ by repeatedly adding or removing a single vertex, using only target sets of $G$, so as to minimize the <i>maximum size</i> of any intermediate target set.
					We prove that it is <b>NP</b>-hard to approximate $\textsf{Minmax Target Set Reconfiguration}$ within a factor of $2-o\left(\frac{1}{\operatorname{polylog} n}\right)$, where $n$ is the number of vertices.
					Our result establishes a tight lower bound on approximability of $\textsf{Minmax Target Set Reconfiguration}$, which admits a $2$-factor approximation algorithm.
					The proof is based on a gap-preserving reduction from $\textsf{Target Set Selection}$ to $\textsf{Minmax Target Set Reconfiguration}$, where <b>NP</b>-hardness of approximation for the former problem is proven by Chen (SIAM J. Discrete Math., 2009) and Charikar, Naamad, and Wirth (APPROX/RANDOM 2016).
				</p>
			</div>

		</ol>



		<hr>

		<h3>Fox Photos Taken By Me</h3>

		<div class="container">
			<div class="row p-2">
				<div class="col">
					<a href="img/fox1.jpg"><img src="img/fox1_small.jpg" class="img-thumbnail"></a>
				</div>
				<div class="col">
					<a href="img/fox2.jpg"><img src="img/fox2_small.jpg" class="img-thumbnail"></a>
				</div>
			</div>
			<div class="row p-2">
				<div class="col">
					<a href="img/fox3.jpg"><img src="img/fox3_small.jpg" class="img-thumbnail"></a>
				</div>
				<div class="col">
					<a href="img/fox4.jpg"><img src="img/fox4_small.jpg" class="img-thumbnail"></a>
				</div>
			</div>
			<div class="row p-2">
				<div class="col">
					<a href="img/fox5.jpg"><img src="img/fox5_small.jpg" class="img-thumbnail"></a>
				</div>
				<div class="col">
					<a href="img/fox6.jpg"><img src="img/fox6_small.jpg" class="img-thumbnail"></a>
				</div>
			</div>
		</div>

		<hr>

		<div class="lead"><a href="./rejections.txt">My Rejection History</a></div>
		<div class="float-end">Last modified: May 4th, 2025</div>
	</div>

	<script src="js/bootstrap.bundle.min.js"></script>
</body>

</html>