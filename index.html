<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Naoto Ohsaka</title>
		<link href="css/bootstrap.min.css" rel="stylesheet">
		<script src="js/bootstrap.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				tex2jax: {
					inlineMath: [ ['$','$'], ['$$','$$'], ["\\(","\\)"] ],
					displayMath: [ ["\\[","\\]"] ]
				}
			});
		</script>
		<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"> </script>
		<meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />
	</head>
	<body>
    <div class="container">
		<h1>Welcome to my homepage !!</h1>
		<h1>Naoto Ohsaka (Ph.D.)</h1>
		<div><a href="./photo.jpg">Photo</a></div>
		<div><a href="./cv.pdf">Curriculum Vitae</a></div>
		<div>Email: naoto.ohsaka (at) gmail.com</div>
		<div>Google Scholar author ID: <a href="https://scholar.google.co.jp/citations?user=Qgkc9DgAAAAJ">Qgkc9DgAAAAJ</a></div>
		<div itemscope itemtype="https://schema.org/Person">ORCID: <a itemprop="sameAs" content="https://orcid.org/0000-0001-9584-4764" href="https://orcid.org/0000-0001-9584-4764" target="orcid.widget" rel="me noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon">0000-0001-9584-4764</a></div>
		<div>DBLP: <a href="https://dblp.org/pid/81/10779.html">81/10779</a></div>
		
		<hr>

		<h2>Research Interests</h2>
		graph algorithms; computational complexity; network diffusion
		
		<hr>

		<h2>Publications</h2>

		<ol>

		<li>
		<span class="label label-info">New!!</span> Spanning Tree Constrained Determinantal Point Processes are Hard to (Approximately) Evaluate.<br>
		Tatsuya Matsuoka,
		<u>Naoto Ohsaka</u>.<br>
		<a href="https://www.journals.elsevier.com/operations-research-letters">Operations Research Letters</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#orl21_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://www.sciencedirect.com/science/article/abs/pii/S0167637721000316">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://arxiv.org/abs/2102.12646">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>ArXiv</span>
		</a>
		</li>
		<div id="orl21_abst" class="collapse"><i>
		We consider determinantal point processes (DPPs) constrained by spanning trees.
		Given a graph $G=(V,E)$ and a positive semi-definite matrix $\mathbf{A}$ indexed by $E$, a spanning-tree DPP defines a distribution such that we draw $S\subseteq E$ with probability proportional to $\det(\mathbf{A}_S)$ only if $S$ induces a spanning tree.
		We prove $\sharp\textsf{P}$-hardness of computing the normalizing constant for spanning-tree DPPs and provide an approximation-preserving reduction from the mixed discriminant, for which FPRAS is not known.
		We show similar results for DPPs constrained by forests.
		</i></div>

		<li>
		<span class="label label-info">New!!</span> Unconstrained MAP Inference, Exponentiated Determinantal Point Processes, and Exponential Inapproximability.<br>
		<u>Naoto Ohsaka</u>.<br>
		<a href="https://aistats.org/aistats2021">24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#aistats21dpp_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="http://proceedings.mlr.press/v130/ohsaka21a.html">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="aistats21dpp_abst" class="collapse"><i>
		We study the computational complexity of two hard problems on determinantal point processes (DPPs).
		One is maximum a posteriori (MAP) inference, i.e., to find a principal submatrix having the maximum determinant.
		The other is probabilistic inference on exponentiated DPPs (E-DPPs), which can sharpen or weaken the diversity preference of DPPs with an exponent parameter $p$.
		We prove the following complexity-theoretic hardness results that explain the difficulty in approximating unconstrained MAP inference and the normalizing constant for E-DPPs.
		<ul>
		<li>
		Unconstrained MAP inference for an $n \times n$ matrix is $\textsf{NP}$-hard to approximate within a $2^{\beta n}$-factor, where $\beta = 10^{-10^{13}} $.
		This result improves upon a $(\frac{9}{8}-\epsilon)$-factor inapproximability given by Kulesza and Taskar (2012).
		</li>
		<li>
		The normalizing constant for E-DPPs of any (fixed) constant exponent $p \geq \beta^{-1} = 10^{10^{13}}$ is $\textsf{NP}$-hard to approximate within a $2^{\beta pn}$-factor.
		This gives a(nother) negative answer to open questions posed by Kulesza andTaskar (2012); Ohsaka and Matsuoka (2020).
		</li>
		</ul>
		</i></div>

		<li>
		<span class="label label-info">New!!</span> Tracking Regret Bounds for Online Submodular Optimization.<br>
		Tatsuya Matsuoka,
		<a href="https://jpn.nec.com/rd/people/shinji_ito.html">Shinji Ito</a>,
		<u>Naoto Ohsaka</u>.<br>
		<a href="https://aistats.org/aistats2021">24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#aistats21track_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="http://proceedings.mlr.press/v130/matsuoka21a.html">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="aistats21track_abst" class="collapse"><i>
		 In this paper, we propose algorithms for online submodular optimization with tracking regret bounds. Online submodular optimization is a generic framework for sequential decision making used to select subsets. Existing algorithms for online submodular optimization have been shown to achieve small (static) regret, which means that the algorithmâ€™s performance is comparable to the performance of a fixed optimal action. Such algorithms, however, may perform poorly in an environment that changes over time. To overcome this problem, we apply a tracking-regret-analysis framework to online submodular optimization, one by which output is assessed through comparison with time-varying optimal subsets. We propose algorithms for submodular minimization, monotone submodular maximization under a size constraint, and unconstrained submodular maximization, and we show tracking regret bounds. In addition, we show that our tracking regret bound for submodular minimization is nearly tight. 
		</i></div>

		<li>
		<span class="label label-info">New!!</span> Predictive Optimization with Zero-Shot Domain Adaptation.<br>
		<a href="https://t-sakai-kure.github.io/">Tomoya Sakai</a>,
		<u>Naoto Ohsaka</u>.<br>
		<a href="https://www.siam.org/conferences/cm/conference/sdm21">2021 SIAM International Conference on Data Mining (SDM 2021)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#sdm21_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<!--
		<a href="">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		-->
		<a href="https://arxiv.org/abs/2101.06233">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>ArXiv</span>
		</a>
		</li>
		<div id="sdm21_abst" class="collapse"><i>
		Prediction in a new domain without any training sample, called zero-shot domain adaptation (ZSDA), is an important task in domain adaptation.
		While prediction in a new domain has gained much attention in recent years, in this paper, we investigate another potential of ZSDA.
		Specifically, instead of predicting responses in a new domain, we find a description of a new domain given a prediction.
		The task is regarded as predictive optimization, but existing predictive optimization methods have not been extended to handling multiple domains.
		We propose a simple framework for predictive optimization with ZSDA and analyze the condition in which the optimization problem becomes convex optimization.
		We also discuss how to handle the interaction of characteristics of a domain in predictive optimization.
		Through numerical experiments, we demonstrate the potential usefulness of our proposed framework. 
		</i></div>

		<li>
		On the (In)tractability of Computing Normalizing Constants for the Product of Determinantal Point Processes.<br>
		<u>Naoto Ohsaka</u>, Tatsuya Matsuoka.<br>
		<a href="https://icml.cc/Conferences/2020/">37th International Conference on Machine Learning (ICML 2020)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#icml20_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="http://proceedings.mlr.press/v119/ohsaka20a.html">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="icml20_abst" class="collapse"><i>
		We consider the product of determinantal point processes (DPPs), a point process whose probability mass is proportional to the product of principal minors of multiple matrices as a natural, promising generalization of DPPs.
		We study the computational complexity of computing its normalizing constant, which is among the most essential probabilistic inference tasks.
		Our complexity-theoretic results (almost) rule out the existence of efficient algorithms for this task, unless input matrices are forced to have favorable structures.
		In particular, we prove the following:
		<ul>
		<li>
		Computing $ \sum_{S} \det(\mathbf{A}_{S,S})^p $ exactly for every (fixed) positive even integer $p$ is $\textsf{UP}$-hard and $\textsf{Mod}_3\textsf{P}$-hard,
		which gives a negative answer to an open question posed by Kulesza and Tasker (2012).
		</li>
		<li>
		$ \sum_{S} \det(\mathbf{A}_{S,S}) \det(\mathbf{B}_{S,S}) \det(\mathbf{C}_{S,S}) $
		is $\textsf{NP}$-hard to approximate within a factor of $ 2^{\mathcal{O}(|\mathcal{I}|^{1-\epsilon})} $ for any $\epsilon > 0$, where $|\mathcal{I}|$ is the input size.
		This result is stronger than $\sharp\textsf{P}$-hardness for the case of two matrices by Gillenwater (2014).
		</li>
		<li>
		There exists a $ k^{\mathcal{O}(k)} |\mathcal{I}|^{\mathcal{O}(1)} $-time algorithm for
		computing $\sum_{S} \det(\mathbf{A}_{S,S}) \det(\mathbf{B}_{S,S})$, where
		$k$ is ``the maximum rank of $\mathbf{A}$ and $\mathbf{B}$'' or
		``the treewidth of the graph induced by nonzero entries of $\mathbf{A}$ and $\mathbf{B}$.''
		Such parameterized algorithms are said to be fixed-parameter tractable.
		</li>
		</ul>
		</i></div>

		<li>
		The Solution Distribution of Influence Maximization: A High-level Experimental Study on Three Algorithmic Approaches.<br>
		<u>Naoto Ohsaka</u>.<br>
		<a href="https://sigmod2020.org/">ACM SIGMOD International Conference on Management of Data 2020 (SIGMOD 2020)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#sigmod20_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://dl.acm.org/doi/10.1145/3318464.3380564">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://arxiv.org/abs/2003.09816">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>ArXiv</span>
		</a>
		</li>
		<div id="sigmod20_abst" class="collapse"><i>
		Influence maximization is among the most fundamental algorithmic problems in social influence analysis.
		Over the last decade, a great effort has been devoted to developing efficient algorithms for influence maximization, so that identifying the ``best'' algorithm has become a demanding task.
		In SIGMOD'17, Arora, Galhotra, and Ranu reported benchmark results on eleven existing algorithms and demonstrated that there is no single state-of-the-art offering the best trade-off between computational efficiency and solution quality.
		<br/>
		In this paper, we report a high-level experimental study on three well-established algorithmic approaches for influence maximization, referred to as Oneshot, Snapshot, and Reverse Influence Sampling (RIS).
		Different from Arora et al., our experimental methodology is so designed that we examine the distribution of random solutions, characterize the relation between the sample number and the actual solution quality, and avoid implementation dependencies.
		Our main findings are as follows:
		1. For a sufficiently large sample number, we obtain a unique solution regardless of algorithms.
		2. The average solution quality of Oneshot, Snapshot, and RIS improves at the same rate up to scaling of sample number.
		3. Oneshot requires more samples than Snapshot, and Snapshot requires fewer but larger samples than RIS.
		We discuss the time efficiency when conditioning Oneshot, Snapshot, and RIS to be of identical accuracy.
 		Our conclusion is that Oneshot is suitable only if the size of available memory is limited, and RIS is more efficient than Snapshot for large networks; Snapshot is preferable for small, low-probability networks.
		</i></div>

		<li>
		A Predictive Optimization Framework for Hierarchical Demand Matching.<br>
		<u>Naoto Ohsaka</u>,
		<a href="https://t-sakai-kure.github.io/">Tomoya Sakai</a>,
		<a href="https://jpn.nec.com/rd/people/akihiro_yabe.html">Akihiro Yabe.</a><br>
		<a href="https://www.siam.org/conferences/cm/conference/sdm20">2020 SIAM International Conference on Data Mining (SDM 2020)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#sdm20_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://epubs.siam.org/doi/10.1137/1.9781611976236.20">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="sdm20_abst" class="collapse"><i>
		Predictive optimization is a framework for designing an entire data-analysis pipeline that comprises both prediction and optimization, to be able to maximize overall throughput performance.
 		In practical demand analysis, a knowledge of hierarchies, which might be geographical or categorical, is recognized as useful, though such additional knowledge has not been taken into account in existing predictive optimization.
		In this paper, we propose a novel hierarchical predictive optimization pipeline that is able to deal with a wide range of applications including inventory management.
		Based on an existing hierarchical demand prediction model, we present a stochastic matching framework that can manage prediction-uncertainty in decision making.
		We further provide a greedy approximation algorithm for solving demand matching on hierarchical structures.
		In experimental evaluations on both artificial and real-world data, we demonstrate the effectiveness of our proposed hierarchical-predictive-optimization pipeline.
		</i></div>

		<li>
		Boosting PageRank Scores by Optimizing Internal Link Structure.<br>
		<u>Naoto Ohsaka</u>,
		Tomohiro Sonobe,
		<a href="http://www.math.keio.ac.jp/~kakimura/index.html">Naonori Kakimura</a>,
		Takuro Fukunaga,
		Sumio Fujita,
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://www.dexa.org/dexa2018">29th International Conference on Database and Expert Systems Applications (DEXA 2018)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#dexa18_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://link.springer.com/chapter/10.1007/978-3-319-98809-2_26">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="dexa18_abst" class="collapse"><i>
		We consider and formulate problems of PageRank score boosting motivated by applications such as effective web advertising.
		More precisely, given a graph and target vertices, one is required to find a fixed-size set of missing edges that maximizes the minimum PageRank score among the targets.
		We provide theoretical analyses to show that all of them are NP-hard.
		To overcome the hardness, we develop heuristic-based algorithms for them.
		We finally perform experiments on several real-world networks to verify the effectiveness of the proposed algorithms compared to baselines.
		Specifically, our algorithm achieves 100 times improvements of the minimum PageRank score among selected 100 vertices by adding only dozens of edges.
		</i></div>

		<li>
		NoSingles: a Space-Efficient Algorithm for Influence Maximization.<br>
		Diana Popova,
		<u>Naoto Ohsaka,</u>
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi,</a>
		<a href="http://webhome.cs.uvic.ca/~thomo/">Alex Thomo.</a><br>
		<a href="http://ssdbm2018.inf.unibz.it/">30th International Conference on Scientific and Statistical Database Management (SSDBM 2018)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#ssdbm18_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://dl.acm.org/citation.cfm?id=3221291">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="ssdbm18_abst" class="collapse"><i>
		Algorithmic problems of computing influence estimation and influence
		maximization have been actively researched for decades. We
		developed a novel algorithm, NoSingles, based on the Reverse Influence
		Sampling method proposed by Borgs et al. in 2013. NoSingles
		solves the problem of influence maximization in large graphs using
		much smaller space than the existing state-of-the-art algorithms
		while preserving the theoretical guarantee of the approximation
		of $$(1-1/e-\epsilon)$$ of the optimum, for any $$ \epsilon > 0 $$. The NoSingles data
		structure is saved on the hard drive of the machine, and can be used
		repeatedly for playing out ``what if'' scenarios (e.g. trying different
		combination of seeds and calculating the influence spread). We
		also introduce a variation of NoSingles algorithm, which further
		decreases the running time, while preserving the approximation
		guarantee. We support our claims with extensive experiments on
		large real-world graphs. Savings in required space allow to successfully
		run NoSingles on a consumer-grade laptop for graphs with
		tens of millions of vertices and hundreds of millions of edges
		</i></div>

		<li>
		 On the Power of Tree-Depth for Fully Polynomial FPT Algorithms.<br>
		<a href="http://research.nii.ac.jp/~yiwata/">Yoichi Iwata</a>,
		Tomoaki Ogasawara,
		<u>Naoto Ohsaka.</u><br>
		<a href="https://stacs2018.sciencesconf.org/">35th International Symposium on Theoretical Aspects of Computer Science (STACS 2018)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#stacs18_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="http://drops.dagstuhl.de/opus/volltexte/2018/8525/pdf/LIPIcs-STACS-2018-41.pdf">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		</li>
		<div id="stacs18_abst" class="collapse"><i>
		There are many classical problems in P whose time complexities have not been improved over the past decades.
		Recent studies of ``Hardness in P'' have revealed that, for several of such problems, the current fastest algorithm is the best possible under some complexity assumptions.
		To bypass this difficulty, Fomin et al. (SODA 2017) introduced the concept of fully polynomial FPT algorithms.
		For a problem with the current best time complexity $$O(n^c)$$, the goal is to design an algorithm running in $$k^{O(1)}n^{c'}$$ time for a parameter $$k$$ and a constant $$c' < c$$. <br>
		In this paper, we investigate the complexity of graph problems in P parameterized by tree-depth, a graph parameter related to tree-width.
		We show that a simple divide-and-conquer method can solve many graph problems, including
		Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle, Replacement Paths, and 2-hop Cover,
		in $$O(\mathrm{td}\cdot m)$$ time or $$O(\mathrm{td}\cdot (m+n\log n))$$ time, where $$\mathrm{td}$$ is the tree-depth of the input graph.
		Because any graph of tree-width $$\mathrm{tw}$$ has tree-depth at most $$(\mathrm{tw}+1)\log_2 n$$, our algorithms also run in $$O(\mathrm{tw}\cdot m\log n)$$ time or $$O(\mathrm{tw}\cdot (m+n\log n)\log n)$$ time.
		These results match or improve the previous best algorithms parameterized by tree-width.
		Especially, we solve an open problem of fully polynomial FPT algorithm for Weighted Matching parameterized by tree-width posed by Fomin et al.
		</i></div>


		<li>
		Coarsening Massive Influence Networks for Scalable Diffusion Analysis.<br>
		<u>Naoto Ohsaka</u>,
		Tomohiro Sonobe,
		Sumio Fujita, and
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://sigmod2017.org/">ACM SIGMOD International Conference on Management of Data 2017 (SIGMOD 2017)</a>. <br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#cmin_sigmod17_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://dl.acm.org/citation.cfm?id=3064045">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/coarsening-massive-influence-networks-for-scalable-diffusion-analysis-sigmod17">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		<a href="https://github.com/chronotable/coarsening-infnet">
			<span class="btn btn-danger btn-xs"><i class="glyphicon glyphicon-cloud"></i>Code</span>
		</a>
		</li>
		<div id="cmin_sigmod17_abst" class="collapse"><i>
			Fueled by the increasing popularity of online social networks, social influence analysis has attracted a great deal of research attention in the past decade.
			The diffusion process is often modeled using influence graphs, and there has been a line of research that involves algorithmic problems in influence graphs.
			However, the vast size of today's real-world networks raises a serious issue with regard to computational efficiency.<br/>

			In this paper, we propose a new algorithm for reducing influence graphs.
			Given an input influence graph, the proposed algorithm produces a vertex-weighted influence graph, which is compact and approximates the diffusion properties of the input graph.
			The central strategy of influence graph reduction is coarsening, which has the potential to greatly reduce the number of edges by merging a vertex set into a single weighted vertex.
			We provide two implementations; a speed-oriented implementation which runs in linear time with linear space and a scalability-oriented implementation which runs in practically linear time with sublinear space.
			Further, we present general frameworks using our compact graphs that accelerate existing algorithms for influence maximization and influence estimation problems, which are motivated by practical applications, such as viral marketing.
			Using these frameworks, we can quickly obtain solutions that have accuracy guarantees under a reasonable assumption.
			Experiments with real-world networks demonstrate that the proposed algorithm can scale to billion-edge graphs and reduce the graph size to up to 4%.
			In addition, our influence maximization framework achieves four times speed-up of a state-of-the-art D-SSA algorithm, and
			our influence estimation framework cuts down the computation time of a simulation-based method to 3.5%.
		</i></div>


		<li>
		Portfolio Optimization for Influence Spread.<br>
		<u>Naoto Ohsaka</u>, and
		<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida.</a><br>
		<a href="http://www.www2017.com.au/">International Conference on World Wide Web (WWW 2017)</a>.<br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#portfolio_www17_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p977.pdf">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/portfolio-optimization-for-influence-spread-www17">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		</li>
		<div id="portfolio_www17_abst" class="collapse"><i>
			Motivated by viral marketing, stochastic diffusion processes that model influence spread on a network have been studied intensively. The primary interest in such models has been to find a seed set of a fixed size that maximizes the expected size of the cascade from it. Practically, however, it is not desirable to have the risk of ending with a small cascade, even if the expected size of the cascade is large. To address this issue, we adopt conditional value at risk (CVaR) as a risk measure, and propose an algorithm that computes a portfolio over seed sets with a provable guarantee on its CVaR. Using real-world social networks, we demonstrate that the portfolio computed by our algorithm has a significantly better CVaR than seed sets computed by other baseline methods.
		</i></div>
		</li>


		<li>
		Maximizing Time-Decaying Influence in Social Networks.<br>
		<u>Naoto Ohsaka</u>,
		<a href="http://www-sys.ist.osaka-u.ac.jp/~ymgc/">Yutaro Yamaguchi</a>,
		<a href="http://www.graco.c.u-tokyo.ac.jp/labs/kakimura/">Naonori Kakimura</a>, and
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://www.ecmlpkdd2016.org/">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2016)</a>.<br>
		<a href="http://link.springer.com/chapter/10.1007/978-3-319-46128-1_9">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/maximizing-timedecaying-influence-in-social-networks-ecmlpkdd16">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		</li>


		<li>
		Dynamic Influence Analysis in Evolving Networks.<br>
		<u>Naoto Ohsaka</u>,
		<a href="http://research.nii.ac.jp/~takiba/">Takuya Akiba</a>, 
		<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a>, and 
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://www.vldb.org/pvldb/vol9.html">Proceedings of the VLDB Endowment (PVLDB 16)</a>.<br>
		<a href="http://www.vldb.org/pvldb/vol9/p1077-ohsaka.pdf">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/dynamic-influence-analysis-in-evolving-networks-vldb16">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		<a href="https://github.com/todo314/dynamic-influence-analysis">
			<span class="btn btn-danger btn-xs"><i class="glyphicon glyphicon-cloud"></i>Code</span>
		</a>
		</li>


		<li>
		Monotone $$k$$-Submodular Function Maximization with Size Constraints.<br>
		<u>Naoto Ohsaka</u>, and
		<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a><br>
		<a href="https://nips.cc/Conferences/2015/">Annual Conference on Neural Information Processing Systems (NIPS 2015)</a> <i>(Poster presentation)</i>.<br>
		<a href="http://papers.nips.cc/paper/5709-monotone-k-submodular-function-maximization-with-size-constraints">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="./slide/ksfm_nips15_poster.pdf">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Poster</span>
		</a>
		</li>


		<li>
		Efficient PageRank Tracking in Evolving Networks.<br>
		<u>Naoto Ohsaka</u>, 
		<a href="http://www.prefield.com/">Takanori Maehara</a>, and 
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://www.kdd.org/kdd2015/">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD 2015)</a> <i>(Research track paper)</i>.<br>
		<a href="http://dl.acm.org/citation.cfm?id=2783297">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/efficient-pagerank-tracking-in-evolving-networks-kdd15">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		</li>


		<li>
		Fast and Accurate Influence Maximization on Large Networks with Pruned Monte-Carlo Simulations.<br>
		<u>Naoto Ohsaka</u>, 
		<a href="http://research.nii.ac.jp/~takiba/">Takuya Akiba</a>, 
		<a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a>, and 
		<a href="http://research.nii.ac.jp/~k_keniti/">Ken-ichi Kawarabayashi.</a><br>
		<a href="http://www.aaai.org/Conferences/AAAI/aaai14.php">AAAI Conference on Artificial Intelligence (AAAI 2014)</a> <i>(Main technical track paper)</i>.<br>
		<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8455">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<a href="https://www.slideshare.net/todo314/fast-and-accurate-influence-maximization-on-large-networks-with-pruned-montecarlo-simulations-aaai14">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		<a href="https://github.com/todo314/pruned-monte-carlo">
			<span class="btn btn-danger btn-xs"><i class="glyphicon glyphicon-cloud"></i>Code</span>
		</a>
		</li>

		<li>
		A Reinforcement Learning Method to Improve the Sweeping Efficiency for an Agent.<br>
		<u>Naoto Ohsaka</u>, 
		<a href="http://tokyo-ct.net/usr/kitakosi/">Daisuke Kitakoshi</a>, and
		Masato Suzuki.<br>
		IEEE International Conference on Granular Computing (GrC 2011).<br>
		<a href="http://ieeexplore.ieee.org/document/6122650/">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Paper</span>
		</a>
		<!--
		<a href="">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		-->
		</li>

		</ol>
		
		<hr>

		<h2>Preprints</h2>

		<ol>
		<li>
		<span class="label label-info">New!!</span> A Fully Polynomial Parameterized Algorithm for Counting the Number of Reachable Vertices in a Digraph.<br>
		<u>Naoto Ohsaka</u>.<br>
		<span class="btn btn-warning btn-xs" data-toggle="collapse" data-target="#fastreach_abst"><i class="glyphicon glyphicon-plus"></i>Abstract</span>
		<a href="https://arxiv.org/abs/2103.04595">
			<span class="btn btn-primary btn-xs"><i class="glyphicon glyphicon-download-alt"></i>ArXiv</span>
		</a>
		</li>
		<div id="fastreach_abst" class="collapse"><i>
		We consider the problem of counting the number of vertices reachable from each vertex in a digraph $G$, which is equal to computing all the out-degrees of the transitive closure of $G$.
		The current (theoretically) fastest algorithms run in quadratic time; however, Borassi has shown that this problem is not solvable in truly subquadratic time unless the Strong Exponential Time Hypothesis fails [Inf. Process. Lett., 116(10):628--630, 2016].
		In this paper, we present an $\mathcal{O}(f^3n)$-time exact algorithm, where $n$ is the number of vertices in $G$ and $f$ is the feedback edge number of $G$.
		Our algorithm thus runs in truly subquadratic time for digraphs of $f=\mathcal{O}(n^{\frac{1}{3}-\epsilon})$ for any $\epsilon > 0$, i.e., the number of edges is $n$ plus $\mathcal{O}(n^{\frac{1}{3}-\epsilon})$, and is fully polynomial fixed parameter tractable, the notion of which was first introduced by  Fomin, Lokshtanov, Pilipczuk, Saurabh, and Wrochna [ACM Trans. Algorithms, 14(3):34:1--34:45, 2018].
		We also show that the same result holds for vertex-weighted digraphs, where the task is to compute the total weights of vertices reachable from each vertex.
		</i></div>
		</ol>

		<hr>

		<h2>Talks</h2>
		<ul>

		<li>
		Monotone $k$-Submodular Function Maximization with Size Constraints. <br>
		23rd March, 2016. <br>
		At <a href="http://www.ml.inf.ethz.ch/">the Institute for Machine Learning at ETH Zurich</a>. <br>
		<a href="./slide/ksfm_ethz16_slide.pdf">
			<span class="btn btn-success btn-xs"><i class="glyphicon glyphicon-download-alt"></i>Slide</span>
		</a>
		</li>

		</ul>
		
		<hr>
		
		<h2>Awards</h2>
		<ul>
		<li>
		<span class="label label-info">New!!</span> DBSJ Kambayashi Young Researcher Award
		</li>
		</ul>

		<h2>Programming Contests</h2>
		<ul>
		<li>
		14th: ACM ICPC 2013 World Finals
		</li>
		</ul>

		<hr>

		<h2>Fox photos taken by me </h2>

		<div class="container">
			<div class="row">
				<div class="col-xs-6">
					<a href="img/fox1.jpg" class="thumbnail"><img src="img/fox1_small.jpg" alt="" /></a>
				</div>
				<div class="col-xs-6">
					<a href="img/fox2.jpg" class="thumbnail"><img src="img/fox2_small.jpg" alt="" /></a>
				</div>
				<div class="col-xs-6">
					<a href="img/fox3.jpg" class="thumbnail"><img src="img/fox3_small.jpg" alt="" /></a>
				</div>
				<div class="col-xs-6">
					<a href="img/fox4.jpg" class="thumbnail"><img src="img/fox4_small.jpg" alt="" /></a>
				</div>
				<div class="col-xs-6">
					<a href="img/fox5.jpg" class="thumbnail"><img src="img/fox5_small.jpg" alt="" /></a>
				</div>
				<div class="col-xs-6">
					<a href="img/fox6.jpg" class="thumbnail"><img src="img/fox6_small.jpg" alt="" /></a>
				</div>
			</div>
		</div>

		<div id="footer" class="navbar">
			<p class="muted pull-right">Last modified: March 23, 2021</p>
		</div>

		<script src="js/bootstrap.min.js"></script>
	</body>
</html>
